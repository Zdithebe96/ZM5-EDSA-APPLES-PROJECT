{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province</th>\n",
       "      <th>Container</th>\n",
       "      <th>Size_Grade</th>\n",
       "      <th>Weight_Kg</th>\n",
       "      <th>Date</th>\n",
       "      <th>Low_Price</th>\n",
       "      <th>High_Price</th>\n",
       "      <th>Sales_Total</th>\n",
       "      <th>Total_Qty_Sold</th>\n",
       "      <th>Total_Kg_Sold</th>\n",
       "      <th>Stock_On_Hand</th>\n",
       "      <th>avg_price_per_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAPE</td>\n",
       "      <td>M4183</td>\n",
       "      <td>1L</td>\n",
       "      <td>18.3</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>150.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>51710.0</td>\n",
       "      <td>332</td>\n",
       "      <td>6075.6</td>\n",
       "      <td>822</td>\n",
       "      <td>8.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CAPE</td>\n",
       "      <td>JG110</td>\n",
       "      <td>2M</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>320</td>\n",
       "      <td>3520.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>W.CAPE-BERGRIVER ETC</td>\n",
       "      <td>JE090</td>\n",
       "      <td>2S</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>18</td>\n",
       "      <td>162.0</td>\n",
       "      <td>1506</td>\n",
       "      <td>6.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>CAPE</td>\n",
       "      <td>M4183</td>\n",
       "      <td>1S</td>\n",
       "      <td>18.3</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>80.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>32020.0</td>\n",
       "      <td>388</td>\n",
       "      <td>7100.4</td>\n",
       "      <td>443</td>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>EASTERN CAPE</td>\n",
       "      <td>IA400</td>\n",
       "      <td>1S</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Province Container Size_Grade  Weight_Kg        Date  \\\n",
       "1                   CAPE     M4183         1L       18.3  2020-09-09   \n",
       "7                   CAPE     JG110         2M       11.0  2020-04-14   \n",
       "24  W.CAPE-BERGRIVER ETC     JE090         2S        9.0  2020-04-16   \n",
       "40                  CAPE     M4183         1S       18.3  2020-05-04   \n",
       "69          EASTERN CAPE     IA400         1S      400.0  2020-09-28   \n",
       "\n",
       "    Low_Price  High_Price  Sales_Total  Total_Qty_Sold  Total_Kg_Sold  \\\n",
       "1       150.0       170.0      51710.0             332         6075.6   \n",
       "7        50.0        50.0      16000.0             320         3520.0   \n",
       "24       55.0        55.0        990.0              18          162.0   \n",
       "40       80.0       120.0      32020.0             388         7100.4   \n",
       "69     1800.0      1800.0       1800.0               1          400.0   \n",
       "\n",
       "    Stock_On_Hand  avg_price_per_kg  \n",
       "1             822              8.51  \n",
       "7               0              4.55  \n",
       "24           1506              6.11  \n",
       "40            443              4.51  \n",
       "69              2              4.50  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "\n",
    "train = pd.read_csv('df - train_set.csv')\n",
    "test = pd.read_csv('df - test_set.csv')\n",
    "\n",
    "train = train[(train['Commodities'] == 'APPLE GOLDEN DELICIOUS')]\n",
    "del train['Commodities'] \n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Province</th>\n",
       "      <th>Container</th>\n",
       "      <th>Size_Grade</th>\n",
       "      <th>Weight_Kg</th>\n",
       "      <th>Date</th>\n",
       "      <th>Low_Price</th>\n",
       "      <th>High_Price</th>\n",
       "      <th>Sales_Total</th>\n",
       "      <th>Total_Qty_Sold</th>\n",
       "      <th>Total_Kg_Sold</th>\n",
       "      <th>Stock_On_Hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>W.CAPE-BERGRIVER ETC</td>\n",
       "      <td>EC120</td>\n",
       "      <td>1M</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>128.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5008.0</td>\n",
       "      <td>38</td>\n",
       "      <td>456.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>W.CAPE-BERGRIVER ETC</td>\n",
       "      <td>M4183</td>\n",
       "      <td>1X</td>\n",
       "      <td>18.3</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>220.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>8</td>\n",
       "      <td>146.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>W.CAPE-BERGRIVER ETC</td>\n",
       "      <td>EC120</td>\n",
       "      <td>1S</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>6</td>\n",
       "      <td>72.0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>W.CAPE-BERGRIVER ETC</td>\n",
       "      <td>M4183</td>\n",
       "      <td>1M</td>\n",
       "      <td>18.3</td>\n",
       "      <td>2020-05-06</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>W.CAPE-BERGRIVER ETC</td>\n",
       "      <td>M4183</td>\n",
       "      <td>1L</td>\n",
       "      <td>18.3</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>140.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>14140.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index              Province Container Size_Grade  Weight_Kg        Date  \\\n",
       "0      1  W.CAPE-BERGRIVER ETC     EC120         1M       12.0  2020-07-09   \n",
       "1      2  W.CAPE-BERGRIVER ETC     M4183         1X       18.3  2020-01-20   \n",
       "2      3  W.CAPE-BERGRIVER ETC     EC120         1S       12.0  2020-08-19   \n",
       "3      4  W.CAPE-BERGRIVER ETC     M4183         1M       18.3  2020-05-06   \n",
       "4      5  W.CAPE-BERGRIVER ETC     M4183         1L       18.3  2020-05-04   \n",
       "\n",
       "   Low_Price  High_Price  Sales_Total  Total_Qty_Sold  Total_Kg_Sold  \\\n",
       "0      128.0       136.0       5008.0              38          456.0   \n",
       "1      220.0       220.0       1760.0               8          146.4   \n",
       "2      120.0       120.0        720.0               6           72.0   \n",
       "3      160.0       160.0        160.0               1           18.3   \n",
       "4      140.0       160.0      14140.0             100         1830.0   \n",
       "\n",
       "   Stock_On_Hand  \n",
       "0              0  \n",
       "1              2  \n",
       "2             45  \n",
       "3              8  \n",
       "4             19  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test['Commodities']\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1952, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 685 entries, 0 to 684\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Index           685 non-null    int64  \n",
      " 1   Province        685 non-null    object \n",
      " 2   Container       685 non-null    object \n",
      " 3   Size_Grade      685 non-null    object \n",
      " 4   Weight_Kg       685 non-null    float64\n",
      " 5   Date            685 non-null    object \n",
      " 6   Low_Price       685 non-null    float64\n",
      " 7   High_Price      685 non-null    float64\n",
      " 8   Sales_Total     685 non-null    float64\n",
      " 9   Total_Qty_Sold  685 non-null    int64  \n",
      " 10  Total_Kg_Sold   685 non-null    float64\n",
      " 11  Stock_On_Hand   685 non-null    int64  \n",
      "dtypes: float64(5), int64(3), object(4)\n",
      "memory usage: 64.3+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight_Kg</th>\n",
       "      <th>Low_Price</th>\n",
       "      <th>High_Price</th>\n",
       "      <th>Sales_Total</th>\n",
       "      <th>Total_Qty_Sold</th>\n",
       "      <th>Total_Kg_Sold</th>\n",
       "      <th>Stock_On_Hand</th>\n",
       "      <th>avg_price_per_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.460912</td>\n",
       "      <td>174.307377</td>\n",
       "      <td>215.648053</td>\n",
       "      <td>20053.533811</td>\n",
       "      <td>174.510758</td>\n",
       "      <td>2960.176332</td>\n",
       "      <td>408.393955</td>\n",
       "      <td>6.778893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>99.655169</td>\n",
       "      <td>373.553578</td>\n",
       "      <td>433.546159</td>\n",
       "      <td>39005.069445</td>\n",
       "      <td>308.810797</td>\n",
       "      <td>6097.416527</td>\n",
       "      <td>724.450582</td>\n",
       "      <td>2.248744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1325.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>219.600000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>5495.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>853.500000</td>\n",
       "      <td>126.500000</td>\n",
       "      <td>6.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.300000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>21082.500000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>3093.525000</td>\n",
       "      <td>468.000000</td>\n",
       "      <td>8.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>3300.000000</td>\n",
       "      <td>369464.000000</td>\n",
       "      <td>4237.000000</td>\n",
       "      <td>74000.000000</td>\n",
       "      <td>6400.000000</td>\n",
       "      <td>21.240000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Weight_Kg    Low_Price   High_Price    Sales_Total  Total_Qty_Sold  \\\n",
       "count  1952.000000  1952.000000  1952.000000    1952.000000     1952.000000   \n",
       "mean     40.460912   174.307377   215.648053   20053.533811      174.510758   \n",
       "std      99.655169   373.553578   433.546159   39005.069445      308.810797   \n",
       "min       3.000000     2.000000     5.000000       5.000000        1.000000   \n",
       "25%       9.000000    50.000000    60.000000    1325.000000       12.000000   \n",
       "50%      12.000000    80.000000   108.000000    5495.000000       64.000000   \n",
       "75%      18.300000   127.250000   160.000000   21082.500000      200.000000   \n",
       "max     400.000000  2300.000000  3300.000000  369464.000000     4237.000000   \n",
       "\n",
       "       Total_Kg_Sold  Stock_On_Hand  avg_price_per_kg  \n",
       "count    1952.000000    1952.000000       1952.000000  \n",
       "mean     2960.176332     408.393955          6.778893  \n",
       "std      6097.416527     724.450582          2.248744  \n",
       "min         3.000000       0.000000          0.250000  \n",
       "25%       219.600000       9.000000          5.460000  \n",
       "50%       853.500000     126.500000          6.670000  \n",
       "75%      3093.525000     468.000000          8.280000  \n",
       "max     74000.000000    6400.000000         21.240000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CAPE', 'W.CAPE-BERGRIVER ETC', 'EASTERN CAPE', 'NATAL',\n",
       "       'WEST COAST', 'TRANSVAAL', 'ORANGE FREE STATE'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Province'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M4183', 'JG110', 'JE090', 'IA400', 'EC120', 'AC030', 'M6125',\n",
       "       'EF120', 'DT063', 'M9125', 'EG140'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Container'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1L', '2M', '2S', '1S', '1M', '1X', '2L', '2U', '2X', '1U'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Size_Grade'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMMY\n",
    "\n",
    "def onehot_encode(df, column):\n",
    "    df = df.copy()\n",
    "    dummies = pd.get_dummies(df[column], prefix=column, drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING \n",
    "seed = 1\n",
    "def preprocess_inputs(df, return_df=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # CLEAN PROVINCE COLUMN\n",
    "    \n",
    "    df['Province'] = df['Province'].str.replace(' ', '_')\n",
    "    df['Province'] = df['Province'].str.replace('.', '_')\n",
    "    df['Province'] = df['Province'].str.replace('-', '_')\n",
    "    \n",
    "    # DATE ENCODING\n",
    "    # Split 'Date' column into year, month and day columns \n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Date'] = df['Date'].dt.strftime('%d.%m.%Y')\n",
    "    df['year'] = pd.DatetimeIndex(df['Date']).year\n",
    "    df['month'] = pd.DatetimeIndex(df['Date']).month\n",
    "    df['day'] = pd.DatetimeIndex(df['Date']).day\n",
    "\n",
    "    df = df.drop(['Date'], axis = 1) \n",
    "    \n",
    "    # BINARY ENCODING\n",
    "    \n",
    "    df['year'] = df['year'].replace({2020: 1, 2019: 0})\n",
    "        \n",
    "    # ONE-HOT ENCODING\n",
    "    for column in ['Province', 'Container']:\n",
    "        df = onehot_encode(df, column)\n",
    "        \n",
    "    # ORDINAL ENCODING\n",
    "    enc = OrdinalEncoder()\n",
    "    df[['Size_Grade']] = enc.fit_transform(df[['Size_Grade']])\n",
    "        \n",
    "    if return_df==True:\n",
    "        ## for training dataset\n",
    "        # REORDER COLUMNS SO THAT OUR DEPENDENT VARIABLE IS THE LAST COLUMN OF THE DATAFRAME\n",
    "        if 'avg_price_per_kg' in df.columns:\n",
    "            column_titles = [col for col in df.columns if col!= 'avg_price_per_kg'] + ['avg_price_per_kg']\n",
    "            df = df.reindex(columns = column_titles)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "        ## for training dataset\n",
    "        # REORDER COLUMNS SO THAT OUR DEPENDENT VARIABLE IS THE LAST COLUMN OF THE DATAFRAME\n",
    "    elif 'avg_price_per_kg' in df.columns:\n",
    "        column_titles = [col for col in df.columns if col!= 'avg_price_per_kg'] + ['avg_price_per_kg']\n",
    "        df = df.reindex(columns = column_titles)\n",
    "    \n",
    "        # SPLIT DATA INTO PREDICTORS AND TARGET\n",
    "    \n",
    "        y = df['avg_price_per_kg']\n",
    "        X = df.drop('avg_price_per_kg', axis=1)\n",
    "        y = np.array(y)\n",
    "        \n",
    "\n",
    "        \n",
    "        # TRAIN TEST SPLIT\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Decision Tree trained.\n",
      "                         Random Forest trained.\n",
      "                     Gradient Boosting trained.\n",
      "                               XGBoost trained.\n",
      "                     CatBoostRegressor trained.\n",
      "                         LGBMRegressor trained.\n"
     ]
    }
   ],
   "source": [
    "# TRAIN A FEW MODELS\n",
    "\n",
    "models = {\n",
    "    \"                         Decision Tree\": DecisionTreeRegressor(random_state= seed),\n",
    "    \"                         Random Forest\": RandomForestRegressor(min_samples_leaf= 1, n_estimators = 500, random_state= seed, max_depth = 13),\n",
    "    \"                     Gradient Boosting\": GradientBoostingRegressor(learning_rate=0.2, n_estimators=6000, random_state= seed, max_depth =2),\n",
    "    \"                               XGBoost\": XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=6000, learning_rate=0.1, random_state= seed),\n",
    "    \"                     CatBoostRegressor\": CatBoostRegressor(verbose=0, learning_rate=0.1, depth = 4, iterations= 7000),\n",
    "    \"                         LGBMRegressor\": LGBMRegressor()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(name + \" trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Decision Tree R^2 Score: 0.87043\n",
      "                         Random Forest R^2 Score: 0.94947\n",
      "                     Gradient Boosting R^2 Score: 0.95698\n",
      "                               XGBoost R^2 Score: 0.96824\n",
      "                     CatBoostRegressor R^2 Score: 0.95384\n",
      "                         LGBMRegressor R^2 Score: 0.94463\n"
     ]
    }
   ],
   "source": [
    "#EVALUATE MODEL ON R SQUARED - HIGHER IS BETTER\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(name + \" R^2 Score: {:.5f}\".format(model.score(X_test, y_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Decision Tree Test RMSE: 0.82309\n",
      "                         Decision Tree Train RMSE: 0.00000\n",
      "Mean Absolute Error: 0.35 Rand.\n",
      "Accuracy: 93.85 %.\n",
      "                         Random Forest Test RMSE: 0.51399\n",
      "                         Random Forest Train RMSE: 0.25615\n",
      "Mean Absolute Error: 0.29 Rand.\n",
      "Accuracy: 94.14 %.\n",
      "                     Gradient Boosting Test RMSE: 0.47429\n",
      "                     Gradient Boosting Train RMSE: 0.02034\n",
      "Mean Absolute Error: 0.29 Rand.\n",
      "Accuracy: 95.12 %.\n",
      "                               XGBoost Test RMSE: 0.40748\n",
      "                               XGBoost Train RMSE: 0.10494\n",
      "Mean Absolute Error: 0.26 Rand.\n",
      "Accuracy: 95.24 %.\n",
      "                     CatBoostRegressor Test RMSE: 0.49127\n",
      "                     CatBoostRegressor Train RMSE: 0.01002\n",
      "Mean Absolute Error: 0.29 Rand.\n",
      "Accuracy: 93.86 %.\n",
      "                         LGBMRegressor Test RMSE: 0.53804\n",
      "                         LGBMRegressor Train RMSE: 0.31678\n",
      "Mean Absolute Error: 0.33 Rand.\n",
      "Accuracy: 93.51 %.\n"
     ]
    }
   ],
   "source": [
    "#EVALUATE MODEL ON RMSE - LOWER IS BETTER\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(name + \" Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,y_pred))))\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print(name + \" Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train))))\n",
    "    \n",
    "    errors = abs(y_pred - y_test)\n",
    "\n",
    "    # Display the performance metrics\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 2), 'Rand.')\n",
    "\n",
    "    mape = np.mean(100 * (errors / y_test))\n",
    "    accuracy = 100 - mape\n",
    "\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CSV FOR SUBMISSION TO KAGGLE\n",
    "# DON'T SUBMIT ALL, ONLY THOSE WHICH DID GOOD ON EVALUATION TO CONFIRM\n",
    "\n",
    "df = preprocess_inputs(test)\n",
    "\n",
    "Xs = list(df.columns)\n",
    "Xs.remove('Index')\n",
    "\n",
    "X_test = df[important_list]\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    d = pd.DataFrame(y_pred, columns =['avg_price_per_kg'])\n",
    "    dff = pd.concat([df['Index'], d], axis=1)\n",
    "    dff = dff.set_index('Index')\n",
    "    \n",
    "    match= re.findall('[A-Z]', name)    \n",
    "    matchno = re.findall('[0-9]+', name)   \n",
    "    file_name = ''.join(match) + ''.join(matchno)\n",
    "    if file_name == 'XGB':\n",
    "        dff.to_csv(file_name + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: Weight_Kg            Importance: 0.5899999737739563\n",
      "Variable: Low_Price            Importance: 0.11999999731779099\n",
      "Variable: High_Price           Importance: 0.07999999821186066\n",
      "Variable: Province_NATAL       Importance: 0.07999999821186066\n",
      "Variable: year                 Importance: 0.03999999910593033\n",
      "Variable: Province_W_CAPE_BERGRIVER_ETC Importance: 0.019999999552965164\n",
      "Variable: Size_Grade           Importance: 0.009999999776482582\n",
      "Variable: Sales_Total          Importance: 0.009999999776482582\n",
      "Variable: Total_Kg_Sold        Importance: 0.009999999776482582\n",
      "Variable: Stock_On_Hand        Importance: 0.009999999776482582\n",
      "Variable: Province_EASTERN_CAPE Importance: 0.009999999776482582\n",
      "Variable: Container_M4183      Importance: 0.009999999776482582\n",
      "Variable: Total_Qty_Sold       Importance: 0.0\n",
      "Variable: month                Importance: 0.0\n",
      "Variable: day                  Importance: 0.0\n",
      "Variable: Province_ORANGE_FREE_STATE Importance: 0.0\n",
      "Variable: Province_TRANSVAAL   Importance: 0.0\n",
      "Variable: Province_WEST_COAST  Importance: 0.0\n",
      "Variable: Container_DT063      Importance: 0.0\n",
      "Variable: Container_EC120      Importance: 0.0\n",
      "Variable: Container_EF120      Importance: 0.0\n",
      "Variable: Container_EG140      Importance: 0.0\n",
      "Variable: Container_IA400      Importance: 0.0\n",
      "Variable: Container_JE090      Importance: 0.0\n",
      "Variable: Container_JG110      Importance: 0.0\n",
      "Variable: Container_M6125      Importance: 0.0\n",
      "Variable: Container_M9125      Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# FEAUTURE IMPORTANCES\n",
    "\n",
    "df = train.drop('avg_price_per_kg', axis = 1)\n",
    "features = preprocess_inputs(df)\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == \"                               XGBoost\":\n",
    "        # Get numerical feature importances\n",
    "        importances = list(model.feature_importances_)\n",
    "\n",
    "        # List of tuples with variable and importance\n",
    "        feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "        # Sort the feature importances by most important first\n",
    "        feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        # Print out the feature and importances \n",
    "        [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECTING IMPORTANT FEATURES\n",
    "\n",
    "df = preprocess_inputs(train, return_df=True)\n",
    "\n",
    "\n",
    "y = df['avg_price_per_kg']\n",
    "X = df.drop('avg_price_per_kg', axis=1)\n",
    "\n",
    "# GET IMPORTANT COLUMN NAMES\n",
    "\n",
    "important = []\n",
    "for i in [ 'Total_Kg_Sold', 'Container_IA400', 'Container_M4183', \"Container_JE090\", 'Container_JG110', \n",
    "          'Weight_Kg', 'Total_Qty_Sold', 'High_Price', 'Sales_Total', 'Stock_On_Hand']:\n",
    "    A = [col for col in df.columns if i in col]\n",
    "    important.append(A)\n",
    "    \n",
    "important_list = [item for sublist in important for item in sublist]\n",
    "\n",
    "# IMPORTANT DATAFRAME\n",
    "\n",
    "X_imp = X[important_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained.\n"
     ]
    }
   ],
   "source": [
    "# TEST NEW MODEL WITH IMPORTANT FEATURES ONLY\n",
    "\n",
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "\n",
    "xgb = XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=6000, learning_rate=0.1, random_state= seed)\n",
    "\n",
    "xgb.fit(X_imp_train, y_train)\n",
    "\n",
    "print(\"Trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.24 rand.\n",
      "Accuracy: 95.45 %.\n",
      "Test RMSE: 0.37198\n",
      "Train RMSE: 0.12580\n"
     ]
    }
   ],
   "source": [
    "# CHECK PERFORMANCE METRICS\n",
    "\n",
    "pred = xgb.predict(X_imp_test)\n",
    "\n",
    "errors = abs(pred - y_test)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'rand.')\n",
    "\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = 100 - mape\n",
    "\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "print(\"Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,pred))))\n",
    "y_train_pred = xgb.predict(X_imp_train)\n",
    "print(\"Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare all the models with their previous score rather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Decision Tree trained.\n",
      "                         Random Forest trained.\n",
      "                     Gradient Boosting trained.\n",
      "                               XGBoost trained.\n",
      "                     CatBoostRegressor trained.\n",
      "                         LGBMRegressor trained.\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODELS AGAIN BUT WITH THE SUBSET CREATED ABOVE\n",
    "\n",
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "# TRAIN A FEW MODELS\n",
    "\n",
    "models = {\n",
    "    \"                         Decision Tree\": DecisionTreeRegressor(random_state= seed),\n",
    "    \"                         Random Forest\": RandomForestRegressor(min_samples_leaf= 1, n_estimators = 500, random_state= seed, max_depth = 13),\n",
    "    \"                     Gradient Boosting\": GradientBoostingRegressor(learning_rate=0.2, n_estimators=6000, random_state= seed, max_depth =2),\n",
    "    \"                               XGBoost\": XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=6000, learning_rate=0.1, random_state= seed),\n",
    "    \"                     CatBoostRegressor\": CatBoostRegressor(verbose=0, learning_rate=0.1, depth = 4, iterations= 7000),\n",
    "    \"                         LGBMRegressor\": LGBMRegressor()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_imp_train, y_train)\n",
    "    print(name + \" trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Decision Tree R^2 Score: 0.85566\n",
      "                         Random Forest R^2 Score: 0.90141\n",
      "                     Gradient Boosting R^2 Score: 0.96984\n",
      "                               XGBoost R^2 Score: 0.97354\n",
      "                     CatBoostRegressor R^2 Score: 0.96361\n",
      "                         LGBMRegressor R^2 Score: 0.92886\n"
     ]
    }
   ],
   "source": [
    "#EVALUATE SQUARED - HIGHER IS BETTER\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(name + \" R^2 Score: {:.5f}\".format(model.score(X_imp_test, y_test)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Decision Tree Test RMSE: 0.86874\n",
      "                         Decision Tree Train RMSE: 0.00000\n",
      "Mean Absolute Error: 0.52 Rand.\n",
      "Accuracy: 91.46 %.\n",
      "                         Random Forest Test RMSE: 0.71798\n",
      "                         Random Forest Train RMSE: 0.33643\n",
      "Mean Absolute Error: 0.47 Rand.\n",
      "Accuracy: 91.64 %.\n",
      "                     Gradient Boosting Test RMSE: 0.39711\n",
      "                     Gradient Boosting Train RMSE: 0.03027\n",
      "Mean Absolute Error: 0.26 Rand.\n",
      "Accuracy: 95.52 %.\n",
      "                               XGBoost Test RMSE: 0.37198\n",
      "                               XGBoost Train RMSE: 0.12580\n",
      "Mean Absolute Error: 0.24 Rand.\n",
      "Accuracy: 95.45 %.\n",
      "                     CatBoostRegressor Test RMSE: 0.43619\n",
      "                     CatBoostRegressor Train RMSE: 0.02196\n",
      "Mean Absolute Error: 0.27 Rand.\n",
      "Accuracy: 94.9 %.\n",
      "                         LGBMRegressor Test RMSE: 0.60990\n",
      "                         LGBMRegressor Train RMSE: 0.36697\n",
      "Mean Absolute Error: 0.4 Rand.\n",
      "Accuracy: 92.72 %.\n"
     ]
    }
   ],
   "source": [
    "#EVALUATE MODEL ON RMSE\n",
    "\n",
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_imp_test)\n",
    "    print(name + \" Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,y_pred))))\n",
    "    \n",
    "    y_train_pred = model.predict(X_imp_train)\n",
    "    print(name + \" Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error( y_train,y_train_pred))))\n",
    "    \n",
    "    errors = abs(y_pred - y_test)\n",
    "\n",
    "    # Display the performance metrics\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 2), 'Rand.')\n",
    "\n",
    "    mape = np.mean(100 * (errors / y_test))\n",
    "    accuracy = 100 - mape\n",
    "\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It apears the majority of the best performing models from our initial training have improved. Lets improve them a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW ENSEMBLE STACKING\n",
    "\n",
    "# CHOOSE BEST MODELS FROM EARLIER SCORES\n",
    "\n",
    "gb = GradientBoostingRegressor(learning_rate=0.2, n_estimators=6000, random_state= seed, max_depth =2)\n",
    "xgb = XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=6000, learning_rate=0.1, random_state= seed)\n",
    "cb = CatBoostRegressor(verbose=0, learning_rate=0.1, depth = 4, iterations= 7000, random_state= seed)\n",
    "\n",
    "meta_learner_reg =  XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1, \n",
    "                                 objective='reg:squarederror', n_estimators=6000, learning_rate=0.1, random_state= seed)\n",
    "\n",
    "\n",
    "models_4stacking = [(\"gb\", gb),(\"xgb\", xgb), ('cb', cb)]\n",
    "\n",
    "s_reg = StackingRegressor(estimators=models_4stacking, final_estimator= meta_learner_reg, passthrough = True, cv= 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked model fitted.\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON DATAFRAME WITH 7 COLUMNS(MOST IMPORTANT)\n",
    "\n",
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "\n",
    "s_reg.fit(X_imp_train,y_train)\n",
    "print(\"Stacked model fitted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score:  0.9928457311608316\n",
      "Test RMSE: 0.16630\n",
      "Train RMSE: 0.26121\n",
      "Mean Absolute Error: 0.13 degrees.\n",
      "Accuracy: 97.2 %.\n"
     ]
    }
   ],
   "source": [
    "# METRICS OF STACKING REGRESSOR\n",
    "\n",
    "#X_train, X_test, y_train, y_test = preprocess_inputs(train) 0.41877\n",
    "\n",
    "y_pred = s_reg.predict(X_imp_test)\n",
    "rsq = s_reg.score(X_imp_test, y_test)\n",
    "print(\"R^2 Score: \", rsq)\n",
    "\n",
    "print(\"Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error( y_test,y_pred))))\n",
    "y_train_pred = s_reg.predict(X_imp_train)\n",
    "print(\"Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train))))\n",
    " \n",
    "errors = abs(y_pred - y_test)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_inputs(test)\n",
    "Xs = list(df.columns)\n",
    "Xs.remove('Index')\n",
    "\n",
    "X_test = df[Xs]\n",
    "\n",
    "x_t = X_test\n",
    "\n",
    "y_pred = s_reg.predict(x_t)\n",
    "d = pd.DataFrame(y_pred, columns =['avg_price_per_kg'])\n",
    "dff = pd.concat([df['Index'], d], axis=1)\n",
    "dff = dff.set_index('Index')\n",
    "dff.to_csv('Stackedall.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "important = []\n",
    "for i in ['Low_Price', 'Total_Kg_Sold', 'Container_IA400', 'Weight_Kg', 'Total_Qty_Sold', 'High_Price', 'Sales_Total']:\n",
    "    A = [col for col in df.columns if i in col]\n",
    "    important.append(A)\n",
    "    \n",
    "important_list = [item for sublist in important for item in sublist]\n",
    "\n",
    "# IMPORTANT DATAFRAME\n",
    "X_imp = X[important_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING KFOLD\n",
    "\n",
    "df = preprocess_inputs(train, return_df=True)\n",
    "\n",
    "important = []\n",
    "for i in [ 'Total_Kg_Sold', 'Container_IA400', 'Container_M4183', \"Container_JE090\", 'Container_JG110', \n",
    "          'Weight_Kg', 'Total_Qty_Sold', 'High_Price', 'Sales_Total', 'Stock_On_Hand']:\n",
    "    A = [col for col in df.columns if i in col]\n",
    "    important.append(A)\n",
    "    \n",
    "important_list = [item for sublist in important for item in sublist]\n",
    "\n",
    "# IMPORTANT DATAFRAME\n",
    "X_imp = X[important_list]\n",
    "\n",
    "def split_data_kf(df,K):\n",
    "    \n",
    "    y = df['avg_price_per_kg']\n",
    "    X = df[important_list]  # SUBSET FOR IMPORTANT ROWS\n",
    "    \n",
    "    kf = KFold(n_splits=K, shuffle = False)\n",
    "    indices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        indices.append((train_index, test_index))\n",
    "        \n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_best_kfmodel(df,data_indices):\n",
    "    y = df['avg_price_per_kg']\n",
    "    X = df[important_list]\n",
    "    \n",
    "    RMSE = []\n",
    "    trainRSME =[]\n",
    "    \n",
    "    for (train_indices,test_indices) in data_indices:\n",
    "        X_train, y_train = X.iloc[train_indices,:],y.iloc[train_indices]\n",
    "        X_test, y_test = X.iloc[test_indices,:], y.iloc[test_indices]\n",
    "        \n",
    "        model = StackingRegressor(estimators=models_4stacking, final_estimator= meta_learner_reg, passthrough = True, cv= 4)\n",
    "        \n",
    "        model.fit(X_train, y_train) \n",
    "\n",
    "        y_pred = model.predict(X_test)     \n",
    "        \n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test ,y_pred))\n",
    "        RMSE.append(rmse)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        \n",
    "        train_rsme = np.sqrt(metrics.mean_squared_error( y_train,y_train_pred))\n",
    "        trainRSME.append(train_rsme)\n",
    "        \n",
    "    best = RMSE.index(min(RMSE))\n",
    "    best_indicies = data_indices[best]\n",
    "    \n",
    "    \n",
    "    X_train, y_train = X.iloc[best_indicies[0],:],y.iloc[best_indicies[0]]\n",
    "    X_test, y_test = X.iloc[best_indicies[1],:], y.iloc[best_indicies[1]]\n",
    "    \n",
    "    model = StackingRegressor(estimators=models_4stacking, final_estimator= meta_learner_reg, passthrough = True, cv= 4)\n",
    "    \n",
    "    model.fit(X_train, y_train)       \n",
    "          \n",
    "    return model\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2194026582554057,\n",
       "  0.30308400059963264,\n",
       "  0.2913206263331593,\n",
       "  0.25979543207663186],\n",
       " [0.5238265379372173,\n",
       "  0.36072228144873997,\n",
       "  0.33778788271146826,\n",
       "  0.3803486840052161])"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_inputs(train, return_df = True)\n",
    "data_indices = split_data_kf(df,4)\n",
    "\n",
    "get_best_kfmodel(df,data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = preprocess_inputs(train, return_df = True)\n",
    "data_indices = split_data_kf(train_df,4)\n",
    "model = get_best_kfmodel(train_df,data_indices)\n",
    "\n",
    "df = preprocess_inputs(test)\n",
    "Xs = list(df.columns)\n",
    "Xs.remove('Index')\n",
    "\n",
    "X_test = df[Xs]\n",
    "X_Test_imp = X_test[important_list]\n",
    "\n",
    "y_pred = model.predict(X_Test_imp)\n",
    "d = pd.DataFrame(y_pred, columns =['avg_price_per_kg'])\n",
    "dff = pd.concat([df['Index'], d], axis=1)\n",
    "dff = dff.set_index('Index')\n",
    "dff.to_csv('stackedkfold.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "### XGBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-rmse:3.84734\n",
      "[1]\tTest-rmse:2.63951\n",
      "[2]\tTest-rmse:1.84792\n",
      "[3]\tTest-rmse:1.30097\n",
      "[4]\tTest-rmse:0.91859\n",
      "[5]\tTest-rmse:0.65578\n",
      "[6]\tTest-rmse:0.48766\n",
      "[7]\tTest-rmse:0.38256\n",
      "[8]\tTest-rmse:0.32202\n",
      "[9]\tTest-rmse:0.29785\n",
      "[10]\tTest-rmse:0.29598\n",
      "[11]\tTest-rmse:0.28160\n",
      "[12]\tTest-rmse:0.28086\n",
      "[13]\tTest-rmse:0.28506\n",
      "[14]\tTest-rmse:0.29378\n",
      "[15]\tTest-rmse:0.29053\n",
      "[16]\tTest-rmse:0.29431\n",
      "[17]\tTest-rmse:0.29333\n",
      "[18]\tTest-rmse:0.28909\n",
      "[19]\tTest-rmse:0.28797\n",
      "[20]\tTest-rmse:0.28790\n",
      "[21]\tTest-rmse:0.28190\n",
      "[22]\tTest-rmse:0.27807\n",
      "[23]\tTest-rmse:0.27382\n",
      "[24]\tTest-rmse:0.27366\n",
      "[25]\tTest-rmse:0.26955\n",
      "[26]\tTest-rmse:0.26044\n",
      "[27]\tTest-rmse:0.24641\n",
      "[28]\tTest-rmse:0.24598\n",
      "[29]\tTest-rmse:0.24564\n",
      "[30]\tTest-rmse:0.24544\n",
      "[31]\tTest-rmse:0.24503\n",
      "[32]\tTest-rmse:0.24681\n",
      "[33]\tTest-rmse:0.24739\n",
      "[34]\tTest-rmse:0.24711\n",
      "[35]\tTest-rmse:0.24743\n",
      "[36]\tTest-rmse:0.24964\n",
      "[37]\tTest-rmse:0.25284\n",
      "[38]\tTest-rmse:0.25372\n",
      "[39]\tTest-rmse:0.25356\n",
      "[40]\tTest-rmse:0.25241\n",
      "Best RMSE: 0.25 with 32 rounds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_inputs(train)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squarederror',\n",
    "}\n",
    "\n",
    "params['eval_metric'] = \"rmse\"\n",
    "num_boost_round = 999\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10)\n",
    "\n",
    "\n",
    "print(\"Best RMSE: {:.2f} with {} rounds\".format(\n",
    "                 model.best_score,\n",
    "                 model.best_iteration+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.739938</td>\n",
       "      <td>0.021968</td>\n",
       "      <td>4.743877</td>\n",
       "      <td>0.069658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.377401</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>3.386927</td>\n",
       "      <td>0.067667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.427364</td>\n",
       "      <td>0.016886</td>\n",
       "      <td>2.445935</td>\n",
       "      <td>0.066945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.766519</td>\n",
       "      <td>0.018177</td>\n",
       "      <td>1.800972</td>\n",
       "      <td>0.068197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.311959</td>\n",
       "      <td>0.019460</td>\n",
       "      <td>1.367871</td>\n",
       "      <td>0.073691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.577459</td>\n",
       "      <td>0.098167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.017130</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.577447</td>\n",
       "      <td>0.098183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.016986</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.577454</td>\n",
       "      <td>0.098178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.016729</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.577384</td>\n",
       "      <td>0.098205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.016501</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.577363</td>\n",
       "      <td>0.098212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0           4.739938        0.021968        4.743877       0.069658\n",
       "1           3.377401        0.018440        3.386927       0.067667\n",
       "2           2.427364        0.016886        2.445935       0.066945\n",
       "3           1.766519        0.018177        1.800972       0.068197\n",
       "4           1.311959        0.019460        1.367871       0.073691\n",
       "..               ...             ...             ...            ...\n",
       "161         0.017392        0.000828        0.577459       0.098167\n",
       "162         0.017130        0.000941        0.577447       0.098183\n",
       "163         0.016986        0.000961        0.577454       0.098178\n",
       "164         0.016729        0.000872        0.577384       0.098205\n",
       "165         0.016501        0.000879        0.577363       0.098212\n",
       "\n",
       "[166 rows x 4 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=1,\n",
    "    nfold=4,\n",
    "    metrics={'rmse'},\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57736325"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test-rmse-mean'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(5,15)\n",
    "    for min_child_weight in range(3,7)\n",
    "]\n",
    "\n",
    "# Define initial best params and RMSE\n",
    "min_rmse = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=1,\n",
    "        nfold=5,\n",
    "        metrics={'rmse'},\n",
    "        early_stopping_rounds=10)\n",
    "    \n",
    "    # Update best RMSE\n",
    "    mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "    boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "    print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n",
    "    \n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "        \n",
    "print(\"Best params: {}, {}, RMSE: {}\".format(best_params[0], best_params[1], min_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample=1.0\n",
      "\tRMSE 0.62435875 for 129 rounds\n",
      "Best params: 1.0, 1.0, RMSE: 0.62435875\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [10/10. for i in range(10,11)]\n",
    "    for colsample in [10/10. for i in range(10,11)]\n",
    "]\n",
    "\n",
    "min_rmse = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(subsample, colsample))\n",
    "    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=1,\n",
    "        nfold=4,\n",
    "        metrics={'rmse'},\n",
    "        early_stopping_rounds=10)\n",
    "    \n",
    "    # Update best score\n",
    "    mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "    boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "    print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = (subsample,colsample)\n",
    "print(\"Best params: {}, {}, RMSE: {}\".format(best_params[0], best_params[1], min_rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "CV with eta=0.3\n",
      "Wall time: 582 ms\n",
      "\tRMSE 0.62435875 for 129 rounds\n",
      "\n",
      "CV with eta=0.2\n",
      "Wall time: 1.5 s\n",
      "\tRMSE 0.6204102499999999 for 385 rounds\n",
      "\n",
      "CV with eta=0.1\n",
      "Wall time: 2.33 s\n",
      "\tRMSE 0.6167677500000001 for 600 rounds\n",
      "\n",
      "CV with eta=0.05\n",
      "Wall time: 4.22 s\n",
      "\tRMSE 0.6189087499999999 for 998 rounds\n",
      "\n",
      "CV with eta=0.01\n",
      "Wall time: 3.8 s\n",
      "\tRMSE 0.6918565 for 998 rounds\n",
      "\n",
      "CV with eta=0.005\n",
      "Wall time: 3.84 s\n",
      "\tRMSE 0.7676310000000001 for 998 rounds\n",
      "\n",
      "Best params: 0.1, RMSE: 0.6167677500000001\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "min_rmse = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "    # Run and time CV\n",
    "    \n",
    "    %time cv_results = xgb.cv(params, dtrain,num_boost_round=num_boost_round,seed=1, nfold=4, metrics=['rmse'], early_stopping_rounds=10)\n",
    "    \n",
    "    # Update best score\n",
    "    mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "    boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "    print(\"\\tRMSE {} for {} rounds\\n\".format(mean_rmse, boost_rounds))\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = eta\n",
    "print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3,\n",
       " 'min_child_weight': 6,\n",
       " 'eta': 0.1,\n",
       " 'subsample': 1.0,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'eval_metric': 'rmse'}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['eta'] = .1\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-rmse:6.00210\n",
      "[1]\tTest-rmse:5.42201\n",
      "[2]\tTest-rmse:4.90507\n",
      "[3]\tTest-rmse:4.44281\n",
      "[4]\tTest-rmse:4.02841\n",
      "[5]\tTest-rmse:3.65767\n",
      "[6]\tTest-rmse:3.32693\n",
      "[7]\tTest-rmse:3.03055\n",
      "[8]\tTest-rmse:2.76351\n",
      "[9]\tTest-rmse:2.53166\n",
      "[10]\tTest-rmse:2.32450\n",
      "[11]\tTest-rmse:2.13956\n",
      "[12]\tTest-rmse:1.97227\n",
      "[13]\tTest-rmse:1.82383\n",
      "[14]\tTest-rmse:1.69787\n",
      "[15]\tTest-rmse:1.58494\n",
      "[16]\tTest-rmse:1.48119\n",
      "[17]\tTest-rmse:1.39624\n",
      "[18]\tTest-rmse:1.31767\n",
      "[19]\tTest-rmse:1.24548\n",
      "[20]\tTest-rmse:1.18780\n",
      "[21]\tTest-rmse:1.13900\n",
      "[22]\tTest-rmse:1.09268\n",
      "[23]\tTest-rmse:1.05977\n",
      "[24]\tTest-rmse:1.02049\n",
      "[25]\tTest-rmse:0.98746\n",
      "[26]\tTest-rmse:0.95597\n",
      "[27]\tTest-rmse:0.93827\n",
      "[28]\tTest-rmse:0.91373\n",
      "[29]\tTest-rmse:0.89372\n",
      "[30]\tTest-rmse:0.87661\n",
      "[31]\tTest-rmse:0.85689\n",
      "[32]\tTest-rmse:0.84373\n",
      "[33]\tTest-rmse:0.82737\n",
      "[34]\tTest-rmse:0.81667\n",
      "[35]\tTest-rmse:0.80478\n",
      "[36]\tTest-rmse:0.79101\n",
      "[37]\tTest-rmse:0.77966\n",
      "[38]\tTest-rmse:0.76753\n",
      "[39]\tTest-rmse:0.76075\n",
      "[40]\tTest-rmse:0.75490\n",
      "[41]\tTest-rmse:0.75041\n",
      "[42]\tTest-rmse:0.74318\n",
      "[43]\tTest-rmse:0.73850\n",
      "[44]\tTest-rmse:0.73272\n",
      "[45]\tTest-rmse:0.72608\n",
      "[46]\tTest-rmse:0.71979\n",
      "[47]\tTest-rmse:0.71430\n",
      "[48]\tTest-rmse:0.71326\n",
      "[49]\tTest-rmse:0.71073\n",
      "[50]\tTest-rmse:0.70671\n",
      "[51]\tTest-rmse:0.70023\n",
      "[52]\tTest-rmse:0.69494\n",
      "[53]\tTest-rmse:0.69081\n",
      "[54]\tTest-rmse:0.68754\n",
      "[55]\tTest-rmse:0.68573\n",
      "[56]\tTest-rmse:0.68131\n",
      "[57]\tTest-rmse:0.67677\n",
      "[58]\tTest-rmse:0.67288\n",
      "[59]\tTest-rmse:0.67311\n",
      "[60]\tTest-rmse:0.67200\n",
      "[61]\tTest-rmse:0.66932\n",
      "[62]\tTest-rmse:0.66659\n",
      "[63]\tTest-rmse:0.66353\n",
      "[64]\tTest-rmse:0.65834\n",
      "[65]\tTest-rmse:0.65740\n",
      "[66]\tTest-rmse:0.65545\n",
      "[67]\tTest-rmse:0.65148\n",
      "[68]\tTest-rmse:0.64629\n",
      "[69]\tTest-rmse:0.64324\n",
      "[70]\tTest-rmse:0.64216\n",
      "[71]\tTest-rmse:0.64129\n",
      "[72]\tTest-rmse:0.63899\n",
      "[73]\tTest-rmse:0.63813\n",
      "[74]\tTest-rmse:0.63403\n",
      "[75]\tTest-rmse:0.63209\n",
      "[76]\tTest-rmse:0.63143\n",
      "[77]\tTest-rmse:0.63045\n",
      "[78]\tTest-rmse:0.62984\n",
      "[79]\tTest-rmse:0.62812\n",
      "[80]\tTest-rmse:0.62673\n",
      "[81]\tTest-rmse:0.62448\n",
      "[82]\tTest-rmse:0.62377\n",
      "[83]\tTest-rmse:0.62315\n",
      "[84]\tTest-rmse:0.62233\n",
      "[85]\tTest-rmse:0.62163\n",
      "[86]\tTest-rmse:0.62140\n",
      "[87]\tTest-rmse:0.62020\n",
      "[88]\tTest-rmse:0.62166\n",
      "[89]\tTest-rmse:0.62032\n",
      "[90]\tTest-rmse:0.61903\n",
      "[91]\tTest-rmse:0.61858\n",
      "[92]\tTest-rmse:0.61528\n",
      "[93]\tTest-rmse:0.61497\n",
      "[94]\tTest-rmse:0.61440\n",
      "[95]\tTest-rmse:0.61578\n",
      "[96]\tTest-rmse:0.61526\n",
      "[97]\tTest-rmse:0.61441\n",
      "[98]\tTest-rmse:0.61395\n",
      "[99]\tTest-rmse:0.61321\n",
      "[100]\tTest-rmse:0.61486\n",
      "[101]\tTest-rmse:0.61443\n",
      "[102]\tTest-rmse:0.61432\n",
      "[103]\tTest-rmse:0.61415\n",
      "[104]\tTest-rmse:0.61341\n",
      "[105]\tTest-rmse:0.61279\n",
      "[106]\tTest-rmse:0.61178\n",
      "[107]\tTest-rmse:0.61153\n",
      "[108]\tTest-rmse:0.61095\n",
      "[109]\tTest-rmse:0.61062\n",
      "[110]\tTest-rmse:0.60949\n",
      "[111]\tTest-rmse:0.60870\n",
      "[112]\tTest-rmse:0.60845\n",
      "[113]\tTest-rmse:0.60732\n",
      "[114]\tTest-rmse:0.60717\n",
      "[115]\tTest-rmse:0.60713\n",
      "[116]\tTest-rmse:0.60615\n",
      "[117]\tTest-rmse:0.60493\n",
      "[118]\tTest-rmse:0.60467\n",
      "[119]\tTest-rmse:0.60440\n",
      "[120]\tTest-rmse:0.60362\n",
      "[121]\tTest-rmse:0.60329\n",
      "[122]\tTest-rmse:0.60162\n",
      "[123]\tTest-rmse:0.60163\n",
      "[124]\tTest-rmse:0.60135\n",
      "[125]\tTest-rmse:0.60056\n",
      "[126]\tTest-rmse:0.59888\n",
      "[127]\tTest-rmse:0.59883\n",
      "[128]\tTest-rmse:0.59880\n",
      "[129]\tTest-rmse:0.59813\n",
      "[130]\tTest-rmse:0.59726\n",
      "[131]\tTest-rmse:0.59614\n",
      "[132]\tTest-rmse:0.59575\n",
      "[133]\tTest-rmse:0.59535\n",
      "[134]\tTest-rmse:0.59471\n",
      "[135]\tTest-rmse:0.59426\n",
      "[136]\tTest-rmse:0.59395\n",
      "[137]\tTest-rmse:0.59305\n",
      "[138]\tTest-rmse:0.59292\n",
      "[139]\tTest-rmse:0.59122\n",
      "[140]\tTest-rmse:0.59156\n",
      "[141]\tTest-rmse:0.59137\n",
      "[142]\tTest-rmse:0.59156\n",
      "[143]\tTest-rmse:0.59107\n",
      "[144]\tTest-rmse:0.59028\n",
      "[145]\tTest-rmse:0.59008\n",
      "[146]\tTest-rmse:0.59008\n",
      "[147]\tTest-rmse:0.58836\n",
      "[148]\tTest-rmse:0.58813\n",
      "[149]\tTest-rmse:0.58795\n",
      "[150]\tTest-rmse:0.58769\n",
      "[151]\tTest-rmse:0.58761\n",
      "[152]\tTest-rmse:0.58809\n",
      "[153]\tTest-rmse:0.58753\n",
      "[154]\tTest-rmse:0.58669\n",
      "[155]\tTest-rmse:0.58657\n",
      "[156]\tTest-rmse:0.58626\n",
      "[157]\tTest-rmse:0.58597\n",
      "[158]\tTest-rmse:0.58558\n",
      "[159]\tTest-rmse:0.58525\n",
      "[160]\tTest-rmse:0.58497\n",
      "[161]\tTest-rmse:0.58425\n",
      "[162]\tTest-rmse:0.58395\n",
      "[163]\tTest-rmse:0.58365\n",
      "[164]\tTest-rmse:0.58354\n",
      "[165]\tTest-rmse:0.58223\n",
      "[166]\tTest-rmse:0.58100\n",
      "[167]\tTest-rmse:0.58055\n",
      "[168]\tTest-rmse:0.57973\n",
      "[169]\tTest-rmse:0.57974\n",
      "[170]\tTest-rmse:0.57906\n",
      "[171]\tTest-rmse:0.57904\n",
      "[172]\tTest-rmse:0.57892\n",
      "[173]\tTest-rmse:0.57891\n",
      "[174]\tTest-rmse:0.57639\n",
      "[175]\tTest-rmse:0.57603\n",
      "[176]\tTest-rmse:0.57512\n",
      "[177]\tTest-rmse:0.57485\n",
      "[178]\tTest-rmse:0.57499\n",
      "[179]\tTest-rmse:0.57508\n",
      "[180]\tTest-rmse:0.57493\n",
      "[181]\tTest-rmse:0.57482\n",
      "[182]\tTest-rmse:0.57480\n",
      "[183]\tTest-rmse:0.57470\n",
      "[184]\tTest-rmse:0.57482\n",
      "[185]\tTest-rmse:0.57481\n",
      "[186]\tTest-rmse:0.57249\n",
      "[187]\tTest-rmse:0.57211\n",
      "[188]\tTest-rmse:0.57188\n",
      "[189]\tTest-rmse:0.57022\n",
      "[190]\tTest-rmse:0.56968\n",
      "[191]\tTest-rmse:0.56834\n",
      "[192]\tTest-rmse:0.56804\n",
      "[193]\tTest-rmse:0.56674\n",
      "[194]\tTest-rmse:0.56681\n",
      "[195]\tTest-rmse:0.56689\n",
      "[196]\tTest-rmse:0.56574\n",
      "[197]\tTest-rmse:0.56545\n",
      "[198]\tTest-rmse:0.56470\n",
      "[199]\tTest-rmse:0.56421\n",
      "[200]\tTest-rmse:0.56388\n",
      "[201]\tTest-rmse:0.56317\n",
      "[202]\tTest-rmse:0.56246\n",
      "[203]\tTest-rmse:0.56229\n",
      "[204]\tTest-rmse:0.56236\n",
      "[205]\tTest-rmse:0.56128\n",
      "[206]\tTest-rmse:0.56147\n",
      "[207]\tTest-rmse:0.56149\n",
      "[208]\tTest-rmse:0.56082\n",
      "[209]\tTest-rmse:0.56018\n",
      "[210]\tTest-rmse:0.55975\n",
      "[211]\tTest-rmse:0.55928\n",
      "[212]\tTest-rmse:0.55893\n",
      "[213]\tTest-rmse:0.55868\n",
      "[214]\tTest-rmse:0.55847\n",
      "[215]\tTest-rmse:0.55817\n",
      "[216]\tTest-rmse:0.55822\n",
      "[217]\tTest-rmse:0.55826\n",
      "[218]\tTest-rmse:0.55828\n",
      "[219]\tTest-rmse:0.55768\n",
      "[220]\tTest-rmse:0.55756\n",
      "[221]\tTest-rmse:0.55721\n",
      "[222]\tTest-rmse:0.55716\n",
      "[223]\tTest-rmse:0.55707\n",
      "[224]\tTest-rmse:0.55699\n",
      "[225]\tTest-rmse:0.55593\n",
      "[226]\tTest-rmse:0.55603\n",
      "[227]\tTest-rmse:0.55604\n",
      "[228]\tTest-rmse:0.55539\n",
      "[229]\tTest-rmse:0.55511\n",
      "[230]\tTest-rmse:0.55486\n",
      "[231]\tTest-rmse:0.55468\n",
      "[232]\tTest-rmse:0.55369\n",
      "[233]\tTest-rmse:0.55399\n",
      "[234]\tTest-rmse:0.55354\n",
      "[235]\tTest-rmse:0.55343\n",
      "[236]\tTest-rmse:0.55287\n",
      "[237]\tTest-rmse:0.55306\n",
      "[238]\tTest-rmse:0.55237\n",
      "[239]\tTest-rmse:0.55163\n",
      "[240]\tTest-rmse:0.55060\n",
      "[241]\tTest-rmse:0.55007\n",
      "[242]\tTest-rmse:0.54965\n",
      "[243]\tTest-rmse:0.55012\n",
      "[244]\tTest-rmse:0.54973\n",
      "[245]\tTest-rmse:0.54861\n",
      "[246]\tTest-rmse:0.54882\n",
      "[247]\tTest-rmse:0.54856\n",
      "[248]\tTest-rmse:0.54797\n",
      "[249]\tTest-rmse:0.54808\n",
      "[250]\tTest-rmse:0.54805\n",
      "[251]\tTest-rmse:0.54764\n",
      "[252]\tTest-rmse:0.54751\n",
      "[253]\tTest-rmse:0.54701\n",
      "[254]\tTest-rmse:0.54675\n",
      "[255]\tTest-rmse:0.54670\n",
      "[256]\tTest-rmse:0.54545\n",
      "[257]\tTest-rmse:0.54403\n",
      "[258]\tTest-rmse:0.54389\n",
      "[259]\tTest-rmse:0.54375\n",
      "[260]\tTest-rmse:0.54372\n",
      "[261]\tTest-rmse:0.54327\n",
      "[262]\tTest-rmse:0.54328\n",
      "[263]\tTest-rmse:0.54316\n",
      "[264]\tTest-rmse:0.54319\n",
      "[265]\tTest-rmse:0.54261\n",
      "[266]\tTest-rmse:0.54303\n",
      "[267]\tTest-rmse:0.54272\n",
      "[268]\tTest-rmse:0.54254\n",
      "[269]\tTest-rmse:0.54243\n",
      "[270]\tTest-rmse:0.54203\n",
      "[271]\tTest-rmse:0.54159\n",
      "[272]\tTest-rmse:0.54139\n",
      "[273]\tTest-rmse:0.54138\n",
      "[274]\tTest-rmse:0.54128\n",
      "[275]\tTest-rmse:0.54090\n",
      "[276]\tTest-rmse:0.54010\n",
      "[277]\tTest-rmse:0.54004\n",
      "[278]\tTest-rmse:0.54017\n",
      "[279]\tTest-rmse:0.53970\n",
      "[280]\tTest-rmse:0.53941\n",
      "[281]\tTest-rmse:0.53851\n",
      "[282]\tTest-rmse:0.53845\n",
      "[283]\tTest-rmse:0.53847\n",
      "[284]\tTest-rmse:0.53858\n",
      "[285]\tTest-rmse:0.53774\n",
      "[286]\tTest-rmse:0.53754\n",
      "[287]\tTest-rmse:0.53748\n",
      "[288]\tTest-rmse:0.53712\n",
      "[289]\tTest-rmse:0.53731\n",
      "[290]\tTest-rmse:0.53703\n",
      "[291]\tTest-rmse:0.53685\n",
      "[292]\tTest-rmse:0.53636\n",
      "[293]\tTest-rmse:0.53625\n",
      "[294]\tTest-rmse:0.53592\n",
      "[295]\tTest-rmse:0.53592\n",
      "[296]\tTest-rmse:0.53604\n",
      "[297]\tTest-rmse:0.53622\n",
      "[298]\tTest-rmse:0.53597\n",
      "[299]\tTest-rmse:0.53553\n",
      "[300]\tTest-rmse:0.53561\n",
      "[301]\tTest-rmse:0.53491\n",
      "[302]\tTest-rmse:0.53449\n",
      "[303]\tTest-rmse:0.53440\n",
      "[304]\tTest-rmse:0.53363\n",
      "[305]\tTest-rmse:0.53366\n",
      "[306]\tTest-rmse:0.53373\n",
      "[307]\tTest-rmse:0.53328\n",
      "[308]\tTest-rmse:0.53322\n",
      "[309]\tTest-rmse:0.53266\n",
      "[310]\tTest-rmse:0.53248\n",
      "[311]\tTest-rmse:0.53177\n",
      "[312]\tTest-rmse:0.53184\n",
      "[313]\tTest-rmse:0.53164\n",
      "[314]\tTest-rmse:0.53148\n",
      "[315]\tTest-rmse:0.53142\n",
      "[316]\tTest-rmse:0.53112\n",
      "[317]\tTest-rmse:0.53059\n",
      "[318]\tTest-rmse:0.53046\n",
      "[319]\tTest-rmse:0.52921\n",
      "[320]\tTest-rmse:0.52914\n",
      "[321]\tTest-rmse:0.52917\n",
      "[322]\tTest-rmse:0.52866\n",
      "[323]\tTest-rmse:0.52839\n",
      "[324]\tTest-rmse:0.52798\n",
      "[325]\tTest-rmse:0.52786\n",
      "[326]\tTest-rmse:0.52789\n",
      "[327]\tTest-rmse:0.52779\n",
      "[328]\tTest-rmse:0.52693\n",
      "[329]\tTest-rmse:0.52680\n",
      "[330]\tTest-rmse:0.52663\n",
      "[331]\tTest-rmse:0.52660\n",
      "[332]\tTest-rmse:0.52692\n",
      "[333]\tTest-rmse:0.52691\n",
      "[334]\tTest-rmse:0.52658\n",
      "[335]\tTest-rmse:0.52636\n",
      "[336]\tTest-rmse:0.52606\n",
      "[337]\tTest-rmse:0.52620\n",
      "[338]\tTest-rmse:0.52615\n",
      "[339]\tTest-rmse:0.52610\n",
      "[340]\tTest-rmse:0.52603\n",
      "[341]\tTest-rmse:0.52583\n",
      "[342]\tTest-rmse:0.52579\n",
      "[343]\tTest-rmse:0.52572\n",
      "[344]\tTest-rmse:0.52559\n",
      "[345]\tTest-rmse:0.52592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[346]\tTest-rmse:0.52550\n",
      "[347]\tTest-rmse:0.52507\n",
      "[348]\tTest-rmse:0.52508\n",
      "[349]\tTest-rmse:0.52533\n",
      "[350]\tTest-rmse:0.52528\n",
      "[351]\tTest-rmse:0.52516\n",
      "[352]\tTest-rmse:0.52508\n",
      "[353]\tTest-rmse:0.52478\n",
      "[354]\tTest-rmse:0.52472\n",
      "[355]\tTest-rmse:0.52452\n",
      "[356]\tTest-rmse:0.52440\n",
      "[357]\tTest-rmse:0.52399\n",
      "[358]\tTest-rmse:0.52373\n",
      "[359]\tTest-rmse:0.52354\n",
      "[360]\tTest-rmse:0.52338\n",
      "[361]\tTest-rmse:0.52334\n",
      "[362]\tTest-rmse:0.52317\n",
      "[363]\tTest-rmse:0.52253\n",
      "[364]\tTest-rmse:0.52243\n",
      "[365]\tTest-rmse:0.52232\n",
      "[366]\tTest-rmse:0.52235\n",
      "[367]\tTest-rmse:0.52245\n",
      "[368]\tTest-rmse:0.52246\n",
      "[369]\tTest-rmse:0.52215\n",
      "[370]\tTest-rmse:0.52203\n",
      "[371]\tTest-rmse:0.52199\n",
      "[372]\tTest-rmse:0.52197\n",
      "[373]\tTest-rmse:0.52169\n",
      "[374]\tTest-rmse:0.52160\n",
      "[375]\tTest-rmse:0.52138\n",
      "[376]\tTest-rmse:0.52143\n",
      "[377]\tTest-rmse:0.52132\n",
      "[378]\tTest-rmse:0.52103\n",
      "[379]\tTest-rmse:0.52087\n",
      "[380]\tTest-rmse:0.52074\n",
      "[381]\tTest-rmse:0.52072\n",
      "[382]\tTest-rmse:0.52077\n",
      "[383]\tTest-rmse:0.52070\n",
      "[384]\tTest-rmse:0.52077\n",
      "[385]\tTest-rmse:0.52065\n",
      "[386]\tTest-rmse:0.52031\n",
      "[387]\tTest-rmse:0.51974\n",
      "[388]\tTest-rmse:0.51944\n",
      "[389]\tTest-rmse:0.51944\n",
      "[390]\tTest-rmse:0.51907\n",
      "[391]\tTest-rmse:0.51845\n",
      "[392]\tTest-rmse:0.51838\n",
      "[393]\tTest-rmse:0.51826\n",
      "[394]\tTest-rmse:0.51841\n",
      "[395]\tTest-rmse:0.51852\n",
      "[396]\tTest-rmse:0.51840\n",
      "[397]\tTest-rmse:0.51813\n",
      "[398]\tTest-rmse:0.51805\n",
      "[399]\tTest-rmse:0.51764\n",
      "[400]\tTest-rmse:0.51764\n",
      "[401]\tTest-rmse:0.51772\n",
      "[402]\tTest-rmse:0.51772\n",
      "[403]\tTest-rmse:0.51709\n",
      "[404]\tTest-rmse:0.51710\n",
      "[405]\tTest-rmse:0.51703\n",
      "[406]\tTest-rmse:0.51621\n",
      "[407]\tTest-rmse:0.51575\n",
      "[408]\tTest-rmse:0.51500\n",
      "[409]\tTest-rmse:0.51501\n",
      "[410]\tTest-rmse:0.51489\n",
      "[411]\tTest-rmse:0.51419\n",
      "[412]\tTest-rmse:0.51445\n",
      "[413]\tTest-rmse:0.51438\n",
      "[414]\tTest-rmse:0.51434\n",
      "[415]\tTest-rmse:0.51372\n",
      "[416]\tTest-rmse:0.51379\n",
      "[417]\tTest-rmse:0.51403\n",
      "[418]\tTest-rmse:0.51411\n",
      "[419]\tTest-rmse:0.51427\n",
      "[420]\tTest-rmse:0.51404\n",
      "[421]\tTest-rmse:0.51399\n",
      "[422]\tTest-rmse:0.51350\n",
      "[423]\tTest-rmse:0.51368\n",
      "[424]\tTest-rmse:0.51374\n",
      "[425]\tTest-rmse:0.51378\n",
      "[426]\tTest-rmse:0.51390\n",
      "[427]\tTest-rmse:0.51328\n",
      "[428]\tTest-rmse:0.51338\n",
      "[429]\tTest-rmse:0.51344\n",
      "[430]\tTest-rmse:0.51341\n",
      "[431]\tTest-rmse:0.51352\n",
      "[432]\tTest-rmse:0.51355\n",
      "[433]\tTest-rmse:0.51291\n",
      "[434]\tTest-rmse:0.51277\n",
      "[435]\tTest-rmse:0.51276\n",
      "[436]\tTest-rmse:0.51274\n",
      "[437]\tTest-rmse:0.51293\n",
      "[438]\tTest-rmse:0.51283\n",
      "[439]\tTest-rmse:0.51284\n",
      "[440]\tTest-rmse:0.51286\n",
      "[441]\tTest-rmse:0.51290\n",
      "[442]\tTest-rmse:0.51245\n",
      "[443]\tTest-rmse:0.51253\n",
      "[444]\tTest-rmse:0.51233\n",
      "[445]\tTest-rmse:0.51212\n",
      "[446]\tTest-rmse:0.51168\n",
      "[447]\tTest-rmse:0.51167\n",
      "[448]\tTest-rmse:0.51145\n",
      "[449]\tTest-rmse:0.51133\n",
      "[450]\tTest-rmse:0.51133\n",
      "[451]\tTest-rmse:0.51101\n",
      "[452]\tTest-rmse:0.51105\n",
      "[453]\tTest-rmse:0.51093\n",
      "[454]\tTest-rmse:0.51102\n",
      "[455]\tTest-rmse:0.51056\n",
      "[456]\tTest-rmse:0.51041\n",
      "[457]\tTest-rmse:0.51046\n",
      "[458]\tTest-rmse:0.51023\n",
      "[459]\tTest-rmse:0.50998\n",
      "[460]\tTest-rmse:0.50985\n",
      "[461]\tTest-rmse:0.50977\n",
      "[462]\tTest-rmse:0.50968\n",
      "[463]\tTest-rmse:0.50940\n",
      "[464]\tTest-rmse:0.50894\n",
      "[465]\tTest-rmse:0.50893\n",
      "[466]\tTest-rmse:0.50883\n",
      "[467]\tTest-rmse:0.50845\n",
      "[468]\tTest-rmse:0.50862\n",
      "[469]\tTest-rmse:0.50875\n",
      "[470]\tTest-rmse:0.50872\n",
      "[471]\tTest-rmse:0.50846\n",
      "[472]\tTest-rmse:0.50851\n",
      "[473]\tTest-rmse:0.50858\n",
      "[474]\tTest-rmse:0.50851\n",
      "[475]\tTest-rmse:0.50843\n",
      "[476]\tTest-rmse:0.50840\n",
      "[477]\tTest-rmse:0.50798\n",
      "[478]\tTest-rmse:0.50731\n",
      "[479]\tTest-rmse:0.50724\n",
      "[480]\tTest-rmse:0.50714\n",
      "[481]\tTest-rmse:0.50647\n",
      "[482]\tTest-rmse:0.50647\n",
      "[483]\tTest-rmse:0.50653\n",
      "[484]\tTest-rmse:0.50641\n",
      "[485]\tTest-rmse:0.50634\n",
      "[486]\tTest-rmse:0.50642\n",
      "[487]\tTest-rmse:0.50598\n",
      "[488]\tTest-rmse:0.50586\n",
      "[489]\tTest-rmse:0.50560\n",
      "[490]\tTest-rmse:0.50537\n",
      "[491]\tTest-rmse:0.50530\n",
      "[492]\tTest-rmse:0.50545\n",
      "[493]\tTest-rmse:0.50507\n",
      "[494]\tTest-rmse:0.50482\n",
      "[495]\tTest-rmse:0.50464\n",
      "[496]\tTest-rmse:0.50447\n",
      "[497]\tTest-rmse:0.50432\n",
      "[498]\tTest-rmse:0.50412\n",
      "[499]\tTest-rmse:0.50425\n",
      "[500]\tTest-rmse:0.50435\n",
      "[501]\tTest-rmse:0.50391\n",
      "[502]\tTest-rmse:0.50363\n",
      "[503]\tTest-rmse:0.50355\n",
      "[504]\tTest-rmse:0.50369\n",
      "[505]\tTest-rmse:0.50348\n",
      "[506]\tTest-rmse:0.50336\n",
      "[507]\tTest-rmse:0.50300\n",
      "[508]\tTest-rmse:0.50292\n",
      "[509]\tTest-rmse:0.50285\n",
      "[510]\tTest-rmse:0.50260\n",
      "[511]\tTest-rmse:0.50251\n",
      "[512]\tTest-rmse:0.50260\n",
      "[513]\tTest-rmse:0.50262\n",
      "[514]\tTest-rmse:0.50244\n",
      "[515]\tTest-rmse:0.50232\n",
      "[516]\tTest-rmse:0.50208\n",
      "[517]\tTest-rmse:0.50217\n",
      "[518]\tTest-rmse:0.50140\n",
      "[519]\tTest-rmse:0.50143\n",
      "[520]\tTest-rmse:0.50131\n",
      "[521]\tTest-rmse:0.50116\n",
      "[522]\tTest-rmse:0.50102\n",
      "[523]\tTest-rmse:0.50105\n",
      "[524]\tTest-rmse:0.50101\n",
      "[525]\tTest-rmse:0.50091\n",
      "[526]\tTest-rmse:0.50068\n",
      "[527]\tTest-rmse:0.50050\n",
      "[528]\tTest-rmse:0.50006\n",
      "[529]\tTest-rmse:0.49982\n",
      "[530]\tTest-rmse:0.49987\n",
      "[531]\tTest-rmse:0.49996\n",
      "[532]\tTest-rmse:0.49992\n",
      "[533]\tTest-rmse:0.49996\n",
      "[534]\tTest-rmse:0.49972\n",
      "[535]\tTest-rmse:0.50019\n",
      "[536]\tTest-rmse:0.50014\n",
      "[537]\tTest-rmse:0.50000\n",
      "[538]\tTest-rmse:0.49989\n",
      "[539]\tTest-rmse:0.49999\n",
      "[540]\tTest-rmse:0.49952\n",
      "[541]\tTest-rmse:0.49940\n",
      "[542]\tTest-rmse:0.49955\n",
      "[543]\tTest-rmse:0.49942\n",
      "[544]\tTest-rmse:0.49933\n",
      "[545]\tTest-rmse:0.49901\n",
      "[546]\tTest-rmse:0.49902\n",
      "[547]\tTest-rmse:0.49897\n",
      "[548]\tTest-rmse:0.49906\n",
      "[549]\tTest-rmse:0.49896\n",
      "[550]\tTest-rmse:0.49905\n",
      "[551]\tTest-rmse:0.49894\n",
      "[552]\tTest-rmse:0.49908\n",
      "[553]\tTest-rmse:0.49877\n",
      "[554]\tTest-rmse:0.49874\n",
      "[555]\tTest-rmse:0.49862\n",
      "[556]\tTest-rmse:0.49842\n",
      "[557]\tTest-rmse:0.49839\n",
      "[558]\tTest-rmse:0.49849\n",
      "[559]\tTest-rmse:0.49843\n",
      "[560]\tTest-rmse:0.49837\n",
      "[561]\tTest-rmse:0.49848\n",
      "[562]\tTest-rmse:0.49843\n",
      "[563]\tTest-rmse:0.49844\n",
      "[564]\tTest-rmse:0.49846\n",
      "[565]\tTest-rmse:0.49831\n",
      "[566]\tTest-rmse:0.49826\n",
      "[567]\tTest-rmse:0.49832\n",
      "[568]\tTest-rmse:0.49825\n",
      "[569]\tTest-rmse:0.49789\n",
      "[570]\tTest-rmse:0.49783\n",
      "[571]\tTest-rmse:0.49793\n",
      "[572]\tTest-rmse:0.49788\n",
      "[573]\tTest-rmse:0.49786\n",
      "[574]\tTest-rmse:0.49801\n",
      "[575]\tTest-rmse:0.49791\n",
      "[576]\tTest-rmse:0.49755\n",
      "[577]\tTest-rmse:0.49737\n",
      "[578]\tTest-rmse:0.49727\n",
      "[579]\tTest-rmse:0.49708\n",
      "[580]\tTest-rmse:0.49711\n",
      "[581]\tTest-rmse:0.49699\n",
      "[582]\tTest-rmse:0.49687\n",
      "[583]\tTest-rmse:0.49686\n",
      "[584]\tTest-rmse:0.49694\n",
      "[585]\tTest-rmse:0.49679\n",
      "[586]\tTest-rmse:0.49665\n",
      "[587]\tTest-rmse:0.49662\n",
      "[588]\tTest-rmse:0.49662\n",
      "[589]\tTest-rmse:0.49666\n",
      "[590]\tTest-rmse:0.49673\n",
      "[591]\tTest-rmse:0.49670\n",
      "[592]\tTest-rmse:0.49662\n",
      "[593]\tTest-rmse:0.49660\n",
      "[594]\tTest-rmse:0.49609\n",
      "[595]\tTest-rmse:0.49610\n",
      "[596]\tTest-rmse:0.49611\n",
      "[597]\tTest-rmse:0.49599\n",
      "[598]\tTest-rmse:0.49619\n",
      "[599]\tTest-rmse:0.49614\n",
      "[600]\tTest-rmse:0.49609\n",
      "[601]\tTest-rmse:0.49585\n",
      "[602]\tTest-rmse:0.49576\n",
      "[603]\tTest-rmse:0.49542\n",
      "[604]\tTest-rmse:0.49532\n",
      "[605]\tTest-rmse:0.49500\n",
      "[606]\tTest-rmse:0.49481\n",
      "[607]\tTest-rmse:0.49472\n",
      "[608]\tTest-rmse:0.49459\n",
      "[609]\tTest-rmse:0.49461\n",
      "[610]\tTest-rmse:0.49452\n",
      "[611]\tTest-rmse:0.49454\n",
      "[612]\tTest-rmse:0.49450\n",
      "[613]\tTest-rmse:0.49439\n",
      "[614]\tTest-rmse:0.49439\n",
      "[615]\tTest-rmse:0.49441\n",
      "[616]\tTest-rmse:0.49438\n",
      "[617]\tTest-rmse:0.49450\n",
      "[618]\tTest-rmse:0.49449\n",
      "[619]\tTest-rmse:0.49444\n",
      "[620]\tTest-rmse:0.49416\n",
      "[621]\tTest-rmse:0.49406\n",
      "[622]\tTest-rmse:0.49403\n",
      "[623]\tTest-rmse:0.49402\n",
      "[624]\tTest-rmse:0.49391\n",
      "[625]\tTest-rmse:0.49384\n",
      "[626]\tTest-rmse:0.49377\n",
      "[627]\tTest-rmse:0.49384\n",
      "[628]\tTest-rmse:0.49387\n",
      "[629]\tTest-rmse:0.49382\n",
      "[630]\tTest-rmse:0.49383\n",
      "[631]\tTest-rmse:0.49389\n",
      "[632]\tTest-rmse:0.49387\n",
      "[633]\tTest-rmse:0.49400\n",
      "[634]\tTest-rmse:0.49384\n",
      "[635]\tTest-rmse:0.49380\n",
      "[636]\tTest-rmse:0.49374\n",
      "[637]\tTest-rmse:0.49347\n",
      "[638]\tTest-rmse:0.49349\n",
      "[639]\tTest-rmse:0.49339\n",
      "[640]\tTest-rmse:0.49340\n",
      "[641]\tTest-rmse:0.49323\n",
      "[642]\tTest-rmse:0.49328\n",
      "[643]\tTest-rmse:0.49335\n",
      "[644]\tTest-rmse:0.49336\n",
      "[645]\tTest-rmse:0.49346\n",
      "[646]\tTest-rmse:0.49342\n",
      "[647]\tTest-rmse:0.49330\n",
      "[648]\tTest-rmse:0.49321\n",
      "[649]\tTest-rmse:0.49319\n",
      "[650]\tTest-rmse:0.49330\n",
      "[651]\tTest-rmse:0.49326\n",
      "[652]\tTest-rmse:0.49333\n",
      "[653]\tTest-rmse:0.49332\n",
      "[654]\tTest-rmse:0.49326\n",
      "[655]\tTest-rmse:0.49278\n",
      "[656]\tTest-rmse:0.49277\n",
      "[657]\tTest-rmse:0.49271\n",
      "[658]\tTest-rmse:0.49259\n",
      "[659]\tTest-rmse:0.49261\n",
      "[660]\tTest-rmse:0.49264\n",
      "[661]\tTest-rmse:0.49259\n",
      "[662]\tTest-rmse:0.49261\n",
      "[663]\tTest-rmse:0.49254\n",
      "[664]\tTest-rmse:0.49239\n",
      "[665]\tTest-rmse:0.49243\n",
      "[666]\tTest-rmse:0.49247\n",
      "[667]\tTest-rmse:0.49260\n",
      "[668]\tTest-rmse:0.49245\n",
      "[669]\tTest-rmse:0.49235\n",
      "[670]\tTest-rmse:0.49227\n",
      "[671]\tTest-rmse:0.49232\n",
      "[672]\tTest-rmse:0.49236\n",
      "[673]\tTest-rmse:0.49228\n",
      "[674]\tTest-rmse:0.49237\n",
      "[675]\tTest-rmse:0.49241\n",
      "[676]\tTest-rmse:0.49239\n",
      "[677]\tTest-rmse:0.49230\n",
      "[678]\tTest-rmse:0.49223\n",
      "[679]\tTest-rmse:0.49217\n",
      "[680]\tTest-rmse:0.49206\n",
      "[681]\tTest-rmse:0.49202\n",
      "[682]\tTest-rmse:0.49201\n",
      "[683]\tTest-rmse:0.49213\n",
      "[684]\tTest-rmse:0.49206\n",
      "[685]\tTest-rmse:0.49210\n",
      "[686]\tTest-rmse:0.49200\n",
      "[687]\tTest-rmse:0.49192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[688]\tTest-rmse:0.49187\n",
      "[689]\tTest-rmse:0.49186\n",
      "[690]\tTest-rmse:0.49195\n",
      "[691]\tTest-rmse:0.49184\n",
      "[692]\tTest-rmse:0.49182\n",
      "[693]\tTest-rmse:0.49174\n",
      "[694]\tTest-rmse:0.49163\n",
      "[695]\tTest-rmse:0.49171\n",
      "[696]\tTest-rmse:0.49163\n",
      "[697]\tTest-rmse:0.49159\n",
      "[698]\tTest-rmse:0.49162\n",
      "[699]\tTest-rmse:0.49148\n",
      "[700]\tTest-rmse:0.49130\n",
      "[701]\tTest-rmse:0.49114\n",
      "[702]\tTest-rmse:0.49115\n",
      "[703]\tTest-rmse:0.49109\n",
      "[704]\tTest-rmse:0.49101\n",
      "[705]\tTest-rmse:0.49090\n",
      "[706]\tTest-rmse:0.49111\n",
      "[707]\tTest-rmse:0.49117\n",
      "[708]\tTest-rmse:0.49119\n",
      "[709]\tTest-rmse:0.49116\n",
      "[710]\tTest-rmse:0.49119\n",
      "[711]\tTest-rmse:0.49117\n",
      "[712]\tTest-rmse:0.49110\n",
      "[713]\tTest-rmse:0.49109\n",
      "[714]\tTest-rmse:0.49104\n",
      "[715]\tTest-rmse:0.49102\n",
      "Best RMSE: 0.49 in 706 rounds\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "print(\"Best RMSE: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-rmse:6.00210\n",
      "[1]\tTest-rmse:5.42201\n",
      "[2]\tTest-rmse:4.90507\n",
      "[3]\tTest-rmse:4.44281\n",
      "[4]\tTest-rmse:4.02841\n",
      "[5]\tTest-rmse:3.65767\n",
      "[6]\tTest-rmse:3.32693\n",
      "[7]\tTest-rmse:3.03055\n",
      "[8]\tTest-rmse:2.76351\n",
      "[9]\tTest-rmse:2.53166\n",
      "[10]\tTest-rmse:2.32450\n",
      "[11]\tTest-rmse:2.13956\n",
      "[12]\tTest-rmse:1.97227\n",
      "[13]\tTest-rmse:1.82383\n",
      "[14]\tTest-rmse:1.69787\n",
      "[15]\tTest-rmse:1.58494\n",
      "[16]\tTest-rmse:1.48119\n",
      "[17]\tTest-rmse:1.39624\n",
      "[18]\tTest-rmse:1.31767\n",
      "[19]\tTest-rmse:1.24548\n",
      "[20]\tTest-rmse:1.18780\n",
      "[21]\tTest-rmse:1.13900\n",
      "[22]\tTest-rmse:1.09268\n",
      "[23]\tTest-rmse:1.05977\n",
      "[24]\tTest-rmse:1.02049\n",
      "[25]\tTest-rmse:0.98746\n",
      "[26]\tTest-rmse:0.95597\n",
      "[27]\tTest-rmse:0.93827\n",
      "[28]\tTest-rmse:0.91373\n",
      "[29]\tTest-rmse:0.89372\n",
      "[30]\tTest-rmse:0.87661\n",
      "[31]\tTest-rmse:0.85689\n",
      "[32]\tTest-rmse:0.84373\n",
      "[33]\tTest-rmse:0.82737\n",
      "[34]\tTest-rmse:0.81667\n",
      "[35]\tTest-rmse:0.80478\n",
      "[36]\tTest-rmse:0.79101\n",
      "[37]\tTest-rmse:0.77966\n",
      "[38]\tTest-rmse:0.76753\n",
      "[39]\tTest-rmse:0.76075\n",
      "[40]\tTest-rmse:0.75490\n",
      "[41]\tTest-rmse:0.75041\n",
      "[42]\tTest-rmse:0.74318\n",
      "[43]\tTest-rmse:0.73850\n",
      "[44]\tTest-rmse:0.73272\n",
      "[45]\tTest-rmse:0.72608\n",
      "[46]\tTest-rmse:0.71979\n",
      "[47]\tTest-rmse:0.71430\n",
      "[48]\tTest-rmse:0.71326\n",
      "[49]\tTest-rmse:0.71073\n",
      "[50]\tTest-rmse:0.70671\n",
      "[51]\tTest-rmse:0.70023\n",
      "[52]\tTest-rmse:0.69494\n",
      "[53]\tTest-rmse:0.69081\n",
      "[54]\tTest-rmse:0.68754\n",
      "[55]\tTest-rmse:0.68573\n",
      "[56]\tTest-rmse:0.68131\n",
      "[57]\tTest-rmse:0.67677\n",
      "[58]\tTest-rmse:0.67288\n",
      "[59]\tTest-rmse:0.67311\n",
      "[60]\tTest-rmse:0.67200\n",
      "[61]\tTest-rmse:0.66932\n",
      "[62]\tTest-rmse:0.66659\n",
      "[63]\tTest-rmse:0.66353\n",
      "[64]\tTest-rmse:0.65834\n",
      "[65]\tTest-rmse:0.65740\n",
      "[66]\tTest-rmse:0.65545\n",
      "[67]\tTest-rmse:0.65148\n",
      "[68]\tTest-rmse:0.64629\n",
      "[69]\tTest-rmse:0.64324\n",
      "[70]\tTest-rmse:0.64216\n",
      "[71]\tTest-rmse:0.64129\n",
      "[72]\tTest-rmse:0.63899\n",
      "[73]\tTest-rmse:0.63813\n",
      "[74]\tTest-rmse:0.63403\n",
      "[75]\tTest-rmse:0.63209\n",
      "[76]\tTest-rmse:0.63143\n",
      "[77]\tTest-rmse:0.63045\n",
      "[78]\tTest-rmse:0.62984\n",
      "[79]\tTest-rmse:0.62812\n",
      "[80]\tTest-rmse:0.62673\n",
      "[81]\tTest-rmse:0.62448\n",
      "[82]\tTest-rmse:0.62377\n",
      "[83]\tTest-rmse:0.62315\n",
      "[84]\tTest-rmse:0.62233\n",
      "[85]\tTest-rmse:0.62163\n",
      "[86]\tTest-rmse:0.62140\n",
      "[87]\tTest-rmse:0.62020\n",
      "[88]\tTest-rmse:0.62166\n",
      "[89]\tTest-rmse:0.62032\n",
      "[90]\tTest-rmse:0.61903\n",
      "[91]\tTest-rmse:0.61858\n",
      "[92]\tTest-rmse:0.61528\n",
      "[93]\tTest-rmse:0.61497\n",
      "[94]\tTest-rmse:0.61440\n",
      "[95]\tTest-rmse:0.61578\n",
      "[96]\tTest-rmse:0.61526\n",
      "[97]\tTest-rmse:0.61441\n",
      "[98]\tTest-rmse:0.61395\n",
      "[99]\tTest-rmse:0.61321\n",
      "[100]\tTest-rmse:0.61486\n",
      "[101]\tTest-rmse:0.61443\n",
      "[102]\tTest-rmse:0.61432\n",
      "[103]\tTest-rmse:0.61415\n",
      "[104]\tTest-rmse:0.61341\n",
      "[105]\tTest-rmse:0.61279\n",
      "[106]\tTest-rmse:0.61178\n",
      "[107]\tTest-rmse:0.61153\n",
      "[108]\tTest-rmse:0.61095\n",
      "[109]\tTest-rmse:0.61062\n",
      "[110]\tTest-rmse:0.60949\n",
      "[111]\tTest-rmse:0.60870\n",
      "[112]\tTest-rmse:0.60845\n",
      "[113]\tTest-rmse:0.60732\n",
      "[114]\tTest-rmse:0.60717\n",
      "[115]\tTest-rmse:0.60713\n",
      "[116]\tTest-rmse:0.60615\n",
      "[117]\tTest-rmse:0.60493\n",
      "[118]\tTest-rmse:0.60467\n",
      "[119]\tTest-rmse:0.60440\n",
      "[120]\tTest-rmse:0.60362\n",
      "[121]\tTest-rmse:0.60329\n",
      "[122]\tTest-rmse:0.60162\n",
      "[123]\tTest-rmse:0.60163\n",
      "[124]\tTest-rmse:0.60135\n",
      "[125]\tTest-rmse:0.60056\n",
      "[126]\tTest-rmse:0.59888\n",
      "[127]\tTest-rmse:0.59883\n",
      "[128]\tTest-rmse:0.59880\n",
      "[129]\tTest-rmse:0.59813\n",
      "[130]\tTest-rmse:0.59726\n",
      "[131]\tTest-rmse:0.59614\n",
      "[132]\tTest-rmse:0.59575\n",
      "[133]\tTest-rmse:0.59535\n",
      "[134]\tTest-rmse:0.59471\n",
      "[135]\tTest-rmse:0.59426\n",
      "[136]\tTest-rmse:0.59395\n",
      "[137]\tTest-rmse:0.59305\n",
      "[138]\tTest-rmse:0.59292\n",
      "[139]\tTest-rmse:0.59122\n",
      "[140]\tTest-rmse:0.59156\n",
      "[141]\tTest-rmse:0.59137\n",
      "[142]\tTest-rmse:0.59156\n",
      "[143]\tTest-rmse:0.59107\n",
      "[144]\tTest-rmse:0.59028\n",
      "[145]\tTest-rmse:0.59008\n",
      "[146]\tTest-rmse:0.59008\n",
      "[147]\tTest-rmse:0.58836\n",
      "[148]\tTest-rmse:0.58813\n",
      "[149]\tTest-rmse:0.58795\n",
      "[150]\tTest-rmse:0.58769\n",
      "[151]\tTest-rmse:0.58761\n",
      "[152]\tTest-rmse:0.58809\n",
      "[153]\tTest-rmse:0.58753\n",
      "[154]\tTest-rmse:0.58669\n",
      "[155]\tTest-rmse:0.58657\n",
      "[156]\tTest-rmse:0.58626\n",
      "[157]\tTest-rmse:0.58597\n",
      "[158]\tTest-rmse:0.58558\n",
      "[159]\tTest-rmse:0.58525\n",
      "[160]\tTest-rmse:0.58497\n",
      "[161]\tTest-rmse:0.58425\n",
      "[162]\tTest-rmse:0.58395\n",
      "[163]\tTest-rmse:0.58365\n",
      "[164]\tTest-rmse:0.58354\n",
      "[165]\tTest-rmse:0.58223\n",
      "[166]\tTest-rmse:0.58100\n",
      "[167]\tTest-rmse:0.58055\n",
      "[168]\tTest-rmse:0.57973\n",
      "[169]\tTest-rmse:0.57974\n",
      "[170]\tTest-rmse:0.57906\n",
      "[171]\tTest-rmse:0.57904\n",
      "[172]\tTest-rmse:0.57892\n",
      "[173]\tTest-rmse:0.57891\n",
      "[174]\tTest-rmse:0.57639\n",
      "[175]\tTest-rmse:0.57603\n",
      "[176]\tTest-rmse:0.57512\n",
      "[177]\tTest-rmse:0.57485\n",
      "[178]\tTest-rmse:0.57499\n",
      "[179]\tTest-rmse:0.57508\n",
      "[180]\tTest-rmse:0.57493\n",
      "[181]\tTest-rmse:0.57482\n",
      "[182]\tTest-rmse:0.57480\n",
      "[183]\tTest-rmse:0.57470\n",
      "[184]\tTest-rmse:0.57482\n",
      "[185]\tTest-rmse:0.57481\n",
      "[186]\tTest-rmse:0.57249\n",
      "[187]\tTest-rmse:0.57211\n",
      "[188]\tTest-rmse:0.57188\n",
      "[189]\tTest-rmse:0.57022\n",
      "[190]\tTest-rmse:0.56968\n",
      "[191]\tTest-rmse:0.56834\n",
      "[192]\tTest-rmse:0.56804\n",
      "[193]\tTest-rmse:0.56674\n",
      "[194]\tTest-rmse:0.56681\n",
      "[195]\tTest-rmse:0.56689\n",
      "[196]\tTest-rmse:0.56574\n",
      "[197]\tTest-rmse:0.56545\n",
      "[198]\tTest-rmse:0.56470\n",
      "[199]\tTest-rmse:0.56421\n",
      "[200]\tTest-rmse:0.56388\n",
      "[201]\tTest-rmse:0.56317\n",
      "[202]\tTest-rmse:0.56246\n",
      "[203]\tTest-rmse:0.56229\n",
      "[204]\tTest-rmse:0.56236\n",
      "[205]\tTest-rmse:0.56128\n",
      "[206]\tTest-rmse:0.56147\n",
      "[207]\tTest-rmse:0.56149\n",
      "[208]\tTest-rmse:0.56082\n",
      "[209]\tTest-rmse:0.56018\n",
      "[210]\tTest-rmse:0.55975\n",
      "[211]\tTest-rmse:0.55928\n",
      "[212]\tTest-rmse:0.55893\n",
      "[213]\tTest-rmse:0.55868\n",
      "[214]\tTest-rmse:0.55847\n",
      "[215]\tTest-rmse:0.55817\n",
      "[216]\tTest-rmse:0.55822\n",
      "[217]\tTest-rmse:0.55826\n",
      "[218]\tTest-rmse:0.55828\n",
      "[219]\tTest-rmse:0.55768\n",
      "[220]\tTest-rmse:0.55756\n",
      "[221]\tTest-rmse:0.55721\n",
      "[222]\tTest-rmse:0.55716\n",
      "[223]\tTest-rmse:0.55707\n",
      "[224]\tTest-rmse:0.55699\n",
      "[225]\tTest-rmse:0.55593\n",
      "[226]\tTest-rmse:0.55603\n",
      "[227]\tTest-rmse:0.55604\n",
      "[228]\tTest-rmse:0.55539\n",
      "[229]\tTest-rmse:0.55511\n",
      "[230]\tTest-rmse:0.55486\n",
      "[231]\tTest-rmse:0.55468\n",
      "[232]\tTest-rmse:0.55369\n",
      "[233]\tTest-rmse:0.55399\n",
      "[234]\tTest-rmse:0.55354\n",
      "[235]\tTest-rmse:0.55343\n",
      "[236]\tTest-rmse:0.55287\n",
      "[237]\tTest-rmse:0.55306\n",
      "[238]\tTest-rmse:0.55237\n",
      "[239]\tTest-rmse:0.55163\n",
      "[240]\tTest-rmse:0.55060\n",
      "[241]\tTest-rmse:0.55007\n",
      "[242]\tTest-rmse:0.54965\n",
      "[243]\tTest-rmse:0.55012\n",
      "[244]\tTest-rmse:0.54973\n",
      "[245]\tTest-rmse:0.54861\n",
      "[246]\tTest-rmse:0.54882\n",
      "[247]\tTest-rmse:0.54856\n",
      "[248]\tTest-rmse:0.54797\n",
      "[249]\tTest-rmse:0.54808\n",
      "[250]\tTest-rmse:0.54805\n",
      "[251]\tTest-rmse:0.54764\n",
      "[252]\tTest-rmse:0.54751\n",
      "[253]\tTest-rmse:0.54701\n",
      "[254]\tTest-rmse:0.54675\n",
      "[255]\tTest-rmse:0.54670\n",
      "[256]\tTest-rmse:0.54545\n",
      "[257]\tTest-rmse:0.54403\n",
      "[258]\tTest-rmse:0.54389\n",
      "[259]\tTest-rmse:0.54375\n",
      "[260]\tTest-rmse:0.54372\n",
      "[261]\tTest-rmse:0.54327\n",
      "[262]\tTest-rmse:0.54328\n",
      "[263]\tTest-rmse:0.54316\n",
      "[264]\tTest-rmse:0.54319\n",
      "[265]\tTest-rmse:0.54261\n",
      "[266]\tTest-rmse:0.54303\n",
      "[267]\tTest-rmse:0.54272\n",
      "[268]\tTest-rmse:0.54254\n",
      "[269]\tTest-rmse:0.54243\n",
      "[270]\tTest-rmse:0.54203\n",
      "[271]\tTest-rmse:0.54159\n",
      "[272]\tTest-rmse:0.54139\n",
      "[273]\tTest-rmse:0.54138\n",
      "[274]\tTest-rmse:0.54128\n",
      "[275]\tTest-rmse:0.54090\n",
      "[276]\tTest-rmse:0.54010\n",
      "[277]\tTest-rmse:0.54004\n",
      "[278]\tTest-rmse:0.54017\n",
      "[279]\tTest-rmse:0.53970\n",
      "[280]\tTest-rmse:0.53941\n",
      "[281]\tTest-rmse:0.53851\n",
      "[282]\tTest-rmse:0.53845\n",
      "[283]\tTest-rmse:0.53847\n",
      "[284]\tTest-rmse:0.53858\n",
      "[285]\tTest-rmse:0.53774\n",
      "[286]\tTest-rmse:0.53754\n",
      "[287]\tTest-rmse:0.53748\n",
      "[288]\tTest-rmse:0.53712\n",
      "[289]\tTest-rmse:0.53731\n",
      "[290]\tTest-rmse:0.53703\n",
      "[291]\tTest-rmse:0.53685\n",
      "[292]\tTest-rmse:0.53636\n",
      "[293]\tTest-rmse:0.53625\n",
      "[294]\tTest-rmse:0.53592\n",
      "[295]\tTest-rmse:0.53592\n",
      "[296]\tTest-rmse:0.53604\n",
      "[297]\tTest-rmse:0.53622\n",
      "[298]\tTest-rmse:0.53597\n",
      "[299]\tTest-rmse:0.53553\n",
      "[300]\tTest-rmse:0.53561\n",
      "[301]\tTest-rmse:0.53491\n",
      "[302]\tTest-rmse:0.53449\n",
      "[303]\tTest-rmse:0.53440\n",
      "[304]\tTest-rmse:0.53363\n",
      "[305]\tTest-rmse:0.53366\n",
      "[306]\tTest-rmse:0.53373\n",
      "[307]\tTest-rmse:0.53328\n",
      "[308]\tTest-rmse:0.53322\n",
      "[309]\tTest-rmse:0.53266\n",
      "[310]\tTest-rmse:0.53248\n",
      "[311]\tTest-rmse:0.53177\n",
      "[312]\tTest-rmse:0.53184\n",
      "[313]\tTest-rmse:0.53164\n",
      "[314]\tTest-rmse:0.53148\n",
      "[315]\tTest-rmse:0.53142\n",
      "[316]\tTest-rmse:0.53112\n",
      "[317]\tTest-rmse:0.53059\n",
      "[318]\tTest-rmse:0.53046\n",
      "[319]\tTest-rmse:0.52921\n",
      "[320]\tTest-rmse:0.52914\n",
      "[321]\tTest-rmse:0.52917\n",
      "[322]\tTest-rmse:0.52866\n",
      "[323]\tTest-rmse:0.52839\n",
      "[324]\tTest-rmse:0.52798\n",
      "[325]\tTest-rmse:0.52786\n",
      "[326]\tTest-rmse:0.52789\n",
      "[327]\tTest-rmse:0.52779\n",
      "[328]\tTest-rmse:0.52693\n",
      "[329]\tTest-rmse:0.52680\n",
      "[330]\tTest-rmse:0.52663\n",
      "[331]\tTest-rmse:0.52660\n",
      "[332]\tTest-rmse:0.52692\n",
      "[333]\tTest-rmse:0.52691\n",
      "[334]\tTest-rmse:0.52658\n",
      "[335]\tTest-rmse:0.52636\n",
      "[336]\tTest-rmse:0.52606\n",
      "[337]\tTest-rmse:0.52620\n",
      "[338]\tTest-rmse:0.52615\n",
      "[339]\tTest-rmse:0.52610\n",
      "[340]\tTest-rmse:0.52603\n",
      "[341]\tTest-rmse:0.52583\n",
      "[342]\tTest-rmse:0.52579\n",
      "[343]\tTest-rmse:0.52572\n",
      "[344]\tTest-rmse:0.52559\n",
      "[345]\tTest-rmse:0.52592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[346]\tTest-rmse:0.52550\n",
      "[347]\tTest-rmse:0.52507\n",
      "[348]\tTest-rmse:0.52508\n",
      "[349]\tTest-rmse:0.52533\n",
      "[350]\tTest-rmse:0.52528\n",
      "[351]\tTest-rmse:0.52516\n",
      "[352]\tTest-rmse:0.52508\n",
      "[353]\tTest-rmse:0.52478\n",
      "[354]\tTest-rmse:0.52472\n",
      "[355]\tTest-rmse:0.52452\n",
      "[356]\tTest-rmse:0.52440\n",
      "[357]\tTest-rmse:0.52399\n",
      "[358]\tTest-rmse:0.52373\n",
      "[359]\tTest-rmse:0.52354\n",
      "[360]\tTest-rmse:0.52338\n",
      "[361]\tTest-rmse:0.52334\n",
      "[362]\tTest-rmse:0.52317\n",
      "[363]\tTest-rmse:0.52253\n",
      "[364]\tTest-rmse:0.52243\n",
      "[365]\tTest-rmse:0.52232\n",
      "[366]\tTest-rmse:0.52235\n",
      "[367]\tTest-rmse:0.52245\n",
      "[368]\tTest-rmse:0.52246\n",
      "[369]\tTest-rmse:0.52215\n",
      "[370]\tTest-rmse:0.52203\n",
      "[371]\tTest-rmse:0.52199\n",
      "[372]\tTest-rmse:0.52197\n",
      "[373]\tTest-rmse:0.52169\n",
      "[374]\tTest-rmse:0.52160\n",
      "[375]\tTest-rmse:0.52138\n",
      "[376]\tTest-rmse:0.52143\n",
      "[377]\tTest-rmse:0.52132\n",
      "[378]\tTest-rmse:0.52103\n",
      "[379]\tTest-rmse:0.52087\n",
      "[380]\tTest-rmse:0.52074\n",
      "[381]\tTest-rmse:0.52072\n",
      "[382]\tTest-rmse:0.52077\n",
      "[383]\tTest-rmse:0.52070\n",
      "[384]\tTest-rmse:0.52077\n",
      "[385]\tTest-rmse:0.52065\n",
      "[386]\tTest-rmse:0.52031\n",
      "[387]\tTest-rmse:0.51974\n",
      "[388]\tTest-rmse:0.51944\n",
      "[389]\tTest-rmse:0.51944\n",
      "[390]\tTest-rmse:0.51907\n",
      "[391]\tTest-rmse:0.51845\n",
      "[392]\tTest-rmse:0.51838\n",
      "[393]\tTest-rmse:0.51826\n",
      "[394]\tTest-rmse:0.51841\n",
      "[395]\tTest-rmse:0.51852\n",
      "[396]\tTest-rmse:0.51840\n",
      "[397]\tTest-rmse:0.51813\n",
      "[398]\tTest-rmse:0.51805\n",
      "[399]\tTest-rmse:0.51764\n",
      "[400]\tTest-rmse:0.51764\n",
      "[401]\tTest-rmse:0.51772\n",
      "[402]\tTest-rmse:0.51772\n",
      "[403]\tTest-rmse:0.51709\n",
      "[404]\tTest-rmse:0.51710\n",
      "[405]\tTest-rmse:0.51703\n",
      "[406]\tTest-rmse:0.51621\n",
      "[407]\tTest-rmse:0.51575\n",
      "[408]\tTest-rmse:0.51500\n",
      "[409]\tTest-rmse:0.51501\n",
      "[410]\tTest-rmse:0.51489\n",
      "[411]\tTest-rmse:0.51419\n",
      "[412]\tTest-rmse:0.51445\n",
      "[413]\tTest-rmse:0.51438\n",
      "[414]\tTest-rmse:0.51434\n",
      "[415]\tTest-rmse:0.51372\n",
      "[416]\tTest-rmse:0.51379\n",
      "[417]\tTest-rmse:0.51403\n",
      "[418]\tTest-rmse:0.51411\n",
      "[419]\tTest-rmse:0.51427\n",
      "[420]\tTest-rmse:0.51404\n",
      "[421]\tTest-rmse:0.51399\n",
      "[422]\tTest-rmse:0.51350\n",
      "[423]\tTest-rmse:0.51368\n",
      "[424]\tTest-rmse:0.51374\n",
      "[425]\tTest-rmse:0.51378\n",
      "[426]\tTest-rmse:0.51390\n",
      "[427]\tTest-rmse:0.51328\n",
      "[428]\tTest-rmse:0.51338\n",
      "[429]\tTest-rmse:0.51344\n",
      "[430]\tTest-rmse:0.51341\n",
      "[431]\tTest-rmse:0.51352\n",
      "[432]\tTest-rmse:0.51355\n",
      "[433]\tTest-rmse:0.51291\n",
      "[434]\tTest-rmse:0.51277\n",
      "[435]\tTest-rmse:0.51276\n",
      "[436]\tTest-rmse:0.51274\n",
      "[437]\tTest-rmse:0.51293\n",
      "[438]\tTest-rmse:0.51283\n",
      "[439]\tTest-rmse:0.51284\n",
      "[440]\tTest-rmse:0.51286\n",
      "[441]\tTest-rmse:0.51290\n",
      "[442]\tTest-rmse:0.51245\n",
      "[443]\tTest-rmse:0.51253\n",
      "[444]\tTest-rmse:0.51233\n",
      "[445]\tTest-rmse:0.51212\n",
      "[446]\tTest-rmse:0.51168\n",
      "[447]\tTest-rmse:0.51167\n",
      "[448]\tTest-rmse:0.51145\n",
      "[449]\tTest-rmse:0.51133\n",
      "[450]\tTest-rmse:0.51133\n",
      "[451]\tTest-rmse:0.51101\n",
      "[452]\tTest-rmse:0.51105\n",
      "[453]\tTest-rmse:0.51093\n",
      "[454]\tTest-rmse:0.51102\n",
      "[455]\tTest-rmse:0.51056\n",
      "[456]\tTest-rmse:0.51041\n",
      "[457]\tTest-rmse:0.51046\n",
      "[458]\tTest-rmse:0.51023\n",
      "[459]\tTest-rmse:0.50998\n",
      "[460]\tTest-rmse:0.50985\n",
      "[461]\tTest-rmse:0.50977\n",
      "[462]\tTest-rmse:0.50968\n",
      "[463]\tTest-rmse:0.50940\n",
      "[464]\tTest-rmse:0.50894\n",
      "[465]\tTest-rmse:0.50893\n",
      "[466]\tTest-rmse:0.50883\n",
      "[467]\tTest-rmse:0.50845\n",
      "[468]\tTest-rmse:0.50862\n",
      "[469]\tTest-rmse:0.50875\n",
      "[470]\tTest-rmse:0.50872\n",
      "[471]\tTest-rmse:0.50846\n",
      "[472]\tTest-rmse:0.50851\n",
      "[473]\tTest-rmse:0.50858\n",
      "[474]\tTest-rmse:0.50851\n",
      "[475]\tTest-rmse:0.50843\n",
      "[476]\tTest-rmse:0.50840\n",
      "[477]\tTest-rmse:0.50798\n",
      "[478]\tTest-rmse:0.50731\n",
      "[479]\tTest-rmse:0.50724\n",
      "[480]\tTest-rmse:0.50714\n",
      "[481]\tTest-rmse:0.50647\n",
      "[482]\tTest-rmse:0.50647\n",
      "[483]\tTest-rmse:0.50653\n",
      "[484]\tTest-rmse:0.50641\n",
      "[485]\tTest-rmse:0.50634\n",
      "[486]\tTest-rmse:0.50642\n",
      "[487]\tTest-rmse:0.50598\n",
      "[488]\tTest-rmse:0.50586\n",
      "[489]\tTest-rmse:0.50560\n",
      "[490]\tTest-rmse:0.50537\n",
      "[491]\tTest-rmse:0.50530\n",
      "[492]\tTest-rmse:0.50545\n",
      "[493]\tTest-rmse:0.50507\n",
      "[494]\tTest-rmse:0.50482\n",
      "[495]\tTest-rmse:0.50464\n",
      "[496]\tTest-rmse:0.50447\n",
      "[497]\tTest-rmse:0.50432\n",
      "[498]\tTest-rmse:0.50412\n",
      "[499]\tTest-rmse:0.50425\n",
      "[500]\tTest-rmse:0.50435\n",
      "[501]\tTest-rmse:0.50391\n",
      "[502]\tTest-rmse:0.50363\n",
      "[503]\tTest-rmse:0.50355\n",
      "[504]\tTest-rmse:0.50369\n",
      "[505]\tTest-rmse:0.50348\n",
      "[506]\tTest-rmse:0.50336\n",
      "[507]\tTest-rmse:0.50300\n",
      "[508]\tTest-rmse:0.50292\n",
      "[509]\tTest-rmse:0.50285\n",
      "[510]\tTest-rmse:0.50260\n",
      "[511]\tTest-rmse:0.50251\n",
      "[512]\tTest-rmse:0.50260\n",
      "[513]\tTest-rmse:0.50262\n",
      "[514]\tTest-rmse:0.50244\n",
      "[515]\tTest-rmse:0.50232\n",
      "[516]\tTest-rmse:0.50208\n",
      "[517]\tTest-rmse:0.50217\n",
      "[518]\tTest-rmse:0.50140\n",
      "[519]\tTest-rmse:0.50143\n",
      "[520]\tTest-rmse:0.50131\n",
      "[521]\tTest-rmse:0.50116\n",
      "[522]\tTest-rmse:0.50102\n",
      "[523]\tTest-rmse:0.50105\n",
      "[524]\tTest-rmse:0.50101\n",
      "[525]\tTest-rmse:0.50091\n",
      "[526]\tTest-rmse:0.50068\n",
      "[527]\tTest-rmse:0.50050\n",
      "[528]\tTest-rmse:0.50006\n",
      "[529]\tTest-rmse:0.49982\n",
      "[530]\tTest-rmse:0.49987\n",
      "[531]\tTest-rmse:0.49996\n",
      "[532]\tTest-rmse:0.49992\n",
      "[533]\tTest-rmse:0.49996\n",
      "[534]\tTest-rmse:0.49972\n",
      "[535]\tTest-rmse:0.50019\n",
      "[536]\tTest-rmse:0.50014\n",
      "[537]\tTest-rmse:0.50000\n",
      "[538]\tTest-rmse:0.49989\n",
      "[539]\tTest-rmse:0.49999\n",
      "[540]\tTest-rmse:0.49952\n",
      "[541]\tTest-rmse:0.49940\n",
      "[542]\tTest-rmse:0.49955\n",
      "[543]\tTest-rmse:0.49942\n",
      "[544]\tTest-rmse:0.49933\n",
      "[545]\tTest-rmse:0.49901\n",
      "[546]\tTest-rmse:0.49902\n",
      "[547]\tTest-rmse:0.49897\n",
      "[548]\tTest-rmse:0.49906\n",
      "[549]\tTest-rmse:0.49896\n",
      "[550]\tTest-rmse:0.49905\n",
      "[551]\tTest-rmse:0.49894\n",
      "[552]\tTest-rmse:0.49908\n",
      "[553]\tTest-rmse:0.49877\n",
      "[554]\tTest-rmse:0.49874\n",
      "[555]\tTest-rmse:0.49862\n",
      "[556]\tTest-rmse:0.49842\n",
      "[557]\tTest-rmse:0.49839\n",
      "[558]\tTest-rmse:0.49849\n",
      "[559]\tTest-rmse:0.49843\n",
      "[560]\tTest-rmse:0.49837\n",
      "[561]\tTest-rmse:0.49848\n",
      "[562]\tTest-rmse:0.49843\n",
      "[563]\tTest-rmse:0.49844\n",
      "[564]\tTest-rmse:0.49846\n",
      "[565]\tTest-rmse:0.49831\n",
      "[566]\tTest-rmse:0.49826\n",
      "[567]\tTest-rmse:0.49832\n",
      "[568]\tTest-rmse:0.49825\n",
      "[569]\tTest-rmse:0.49789\n",
      "[570]\tTest-rmse:0.49783\n",
      "[571]\tTest-rmse:0.49793\n",
      "[572]\tTest-rmse:0.49788\n",
      "[573]\tTest-rmse:0.49786\n",
      "[574]\tTest-rmse:0.49801\n",
      "[575]\tTest-rmse:0.49791\n",
      "[576]\tTest-rmse:0.49755\n",
      "[577]\tTest-rmse:0.49737\n",
      "[578]\tTest-rmse:0.49727\n",
      "[579]\tTest-rmse:0.49708\n",
      "[580]\tTest-rmse:0.49711\n",
      "[581]\tTest-rmse:0.49699\n",
      "[582]\tTest-rmse:0.49687\n",
      "[583]\tTest-rmse:0.49686\n",
      "[584]\tTest-rmse:0.49694\n",
      "[585]\tTest-rmse:0.49679\n",
      "[586]\tTest-rmse:0.49665\n",
      "[587]\tTest-rmse:0.49662\n",
      "[588]\tTest-rmse:0.49662\n",
      "[589]\tTest-rmse:0.49666\n",
      "[590]\tTest-rmse:0.49673\n",
      "[591]\tTest-rmse:0.49670\n",
      "[592]\tTest-rmse:0.49662\n",
      "[593]\tTest-rmse:0.49660\n",
      "[594]\tTest-rmse:0.49609\n",
      "[595]\tTest-rmse:0.49610\n",
      "[596]\tTest-rmse:0.49611\n",
      "[597]\tTest-rmse:0.49599\n",
      "[598]\tTest-rmse:0.49619\n",
      "[599]\tTest-rmse:0.49614\n",
      "[600]\tTest-rmse:0.49609\n",
      "[601]\tTest-rmse:0.49585\n",
      "[602]\tTest-rmse:0.49576\n",
      "[603]\tTest-rmse:0.49542\n",
      "[604]\tTest-rmse:0.49532\n",
      "[605]\tTest-rmse:0.49500\n",
      "[606]\tTest-rmse:0.49481\n",
      "[607]\tTest-rmse:0.49472\n",
      "[608]\tTest-rmse:0.49459\n",
      "[609]\tTest-rmse:0.49461\n",
      "[610]\tTest-rmse:0.49452\n",
      "[611]\tTest-rmse:0.49454\n",
      "[612]\tTest-rmse:0.49450\n",
      "[613]\tTest-rmse:0.49439\n",
      "[614]\tTest-rmse:0.49439\n",
      "[615]\tTest-rmse:0.49441\n",
      "[616]\tTest-rmse:0.49438\n",
      "[617]\tTest-rmse:0.49450\n",
      "[618]\tTest-rmse:0.49449\n",
      "[619]\tTest-rmse:0.49444\n",
      "[620]\tTest-rmse:0.49416\n",
      "[621]\tTest-rmse:0.49406\n",
      "[622]\tTest-rmse:0.49403\n",
      "[623]\tTest-rmse:0.49402\n",
      "[624]\tTest-rmse:0.49391\n",
      "[625]\tTest-rmse:0.49384\n",
      "[626]\tTest-rmse:0.49377\n",
      "[627]\tTest-rmse:0.49384\n",
      "[628]\tTest-rmse:0.49387\n",
      "[629]\tTest-rmse:0.49382\n",
      "[630]\tTest-rmse:0.49383\n",
      "[631]\tTest-rmse:0.49389\n",
      "[632]\tTest-rmse:0.49387\n",
      "[633]\tTest-rmse:0.49400\n",
      "[634]\tTest-rmse:0.49384\n",
      "[635]\tTest-rmse:0.49380\n",
      "[636]\tTest-rmse:0.49374\n",
      "[637]\tTest-rmse:0.49347\n",
      "[638]\tTest-rmse:0.49349\n",
      "[639]\tTest-rmse:0.49339\n",
      "[640]\tTest-rmse:0.49340\n",
      "[641]\tTest-rmse:0.49323\n",
      "[642]\tTest-rmse:0.49328\n",
      "[643]\tTest-rmse:0.49335\n",
      "[644]\tTest-rmse:0.49336\n",
      "[645]\tTest-rmse:0.49346\n",
      "[646]\tTest-rmse:0.49342\n",
      "[647]\tTest-rmse:0.49330\n",
      "[648]\tTest-rmse:0.49321\n",
      "[649]\tTest-rmse:0.49319\n",
      "[650]\tTest-rmse:0.49330\n",
      "[651]\tTest-rmse:0.49326\n",
      "[652]\tTest-rmse:0.49333\n",
      "[653]\tTest-rmse:0.49332\n",
      "[654]\tTest-rmse:0.49326\n",
      "[655]\tTest-rmse:0.49278\n",
      "[656]\tTest-rmse:0.49277\n",
      "[657]\tTest-rmse:0.49271\n",
      "[658]\tTest-rmse:0.49259\n",
      "[659]\tTest-rmse:0.49261\n",
      "[660]\tTest-rmse:0.49264\n",
      "[661]\tTest-rmse:0.49259\n",
      "[662]\tTest-rmse:0.49261\n",
      "[663]\tTest-rmse:0.49254\n",
      "[664]\tTest-rmse:0.49239\n",
      "[665]\tTest-rmse:0.49243\n",
      "[666]\tTest-rmse:0.49247\n",
      "[667]\tTest-rmse:0.49260\n",
      "[668]\tTest-rmse:0.49245\n",
      "[669]\tTest-rmse:0.49235\n",
      "[670]\tTest-rmse:0.49227\n",
      "[671]\tTest-rmse:0.49232\n",
      "[672]\tTest-rmse:0.49236\n",
      "[673]\tTest-rmse:0.49228\n",
      "[674]\tTest-rmse:0.49237\n",
      "[675]\tTest-rmse:0.49241\n",
      "[676]\tTest-rmse:0.49239\n",
      "[677]\tTest-rmse:0.49230\n",
      "[678]\tTest-rmse:0.49223\n",
      "[679]\tTest-rmse:0.49217\n",
      "[680]\tTest-rmse:0.49206\n",
      "[681]\tTest-rmse:0.49202\n",
      "[682]\tTest-rmse:0.49201\n",
      "[683]\tTest-rmse:0.49213\n",
      "[684]\tTest-rmse:0.49206\n",
      "[685]\tTest-rmse:0.49210\n",
      "[686]\tTest-rmse:0.49200\n",
      "[687]\tTest-rmse:0.49192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[688]\tTest-rmse:0.49187\n",
      "[689]\tTest-rmse:0.49186\n",
      "[690]\tTest-rmse:0.49195\n",
      "[691]\tTest-rmse:0.49184\n",
      "[692]\tTest-rmse:0.49182\n",
      "[693]\tTest-rmse:0.49174\n",
      "[694]\tTest-rmse:0.49163\n",
      "[695]\tTest-rmse:0.49171\n",
      "[696]\tTest-rmse:0.49163\n",
      "[697]\tTest-rmse:0.49159\n",
      "[698]\tTest-rmse:0.49162\n",
      "[699]\tTest-rmse:0.49148\n",
      "[700]\tTest-rmse:0.49130\n",
      "[701]\tTest-rmse:0.49114\n",
      "[702]\tTest-rmse:0.49115\n",
      "[703]\tTest-rmse:0.49109\n",
      "[704]\tTest-rmse:0.49101\n",
      "[705]\tTest-rmse:0.49090\n"
     ]
    }
   ],
   "source": [
    "num_boost_round = model.best_iteration + 1\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.49090\n",
      "Train RMSE: 0.16178\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(dtest)\n",
    "print(\"Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,y_pred))))\n",
    "    \n",
    "y_train_pred = best_model.predict(dtrain)\n",
    "print(\"Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3,\n",
       " 'min_child_weight': 6,\n",
       " 'eta': 0.1,\n",
       " 'subsample': 1.0,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'eval_metric': 'rmse'}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params\n",
    "#num_boost_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(train)\n",
    "\n",
    "max_depth = 3\n",
    "min_child_weight = 6\n",
    "subsample = 1\n",
    "colsample_bytree = 1\n",
    "objective = 'reg:squarederror'\n",
    "num_estimators = 706\n",
    "learning_rate = 0.1\n",
    "\n",
    "xgb = XGBRegressor(max_depth=3,\n",
    "                min_child_weight=6,\n",
    "                subsample=1,\n",
    "                colsample_bytree=1,\n",
    "                objective='reg:squarederror',\n",
    "                n_estimators=706,\n",
    "                learning_rate=0.1, random_state= seed)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "print('trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.49090\n",
      "Train RMSE: 0.16178\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb.predict(X_test)\n",
    "print(\"Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,y_pred))))\n",
    "    \n",
    "y_train_pred = xgb.predict(X_train)\n",
    "print(\"Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(max_depth=3,min_child_weight=3,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=442, learning_rate=0.1, random_state= seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
